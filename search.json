[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAX712X: Software development for data science",
    "section": "",
    "text": "(Almost) everything you need to know as an applied mathematician/statistician concerning coding and system administration.\n\n\n\nJoseph Salmon : joseph.salmon@umontpellier.fr,\nBenjamin Charlier : benjamin.charlier@umontpellier.fr\n\nThis course material was improved with the help of some students including:\n\nAmelie Vernay\nTanguy Lefort\n\n\n\n\nStudents are expected to know basic notions of probabilities, optimization, linear algebra and statistics for this course. Some rudiments on coding are also expected (if, for, while, functions) but not mandatory.\n\n\n\nThis course focuses on discovering good coding practices (the language used is Python, but some elements of bash and git will also be useful) for professional coding. A special focus on data processing and visualization will be at the heart of the course. We will mostly focus on basic programming concepts, as well as on discovering the Python scientific libraries, including numpy, scipy, pandas, matplotlib, seaborn. Beyond pandas ninja skills, we will also introduce modern practices for coders : (unitary) tests, version control, documentation generation, etc.\n\nBC : (09/09/2022) Introduction to Linux essentials and command line tools: regexp, grep, find, rename\nBC : (16/09/2022) IDE: VScode, Python virtual env: Anaconda, Python virtual environment, terminal, etc.\nBC : (23/09/2022) Git: a first introduction, github, ssh key creation, various git commands, conflict, pull request; see also Bonus/, hands on git\nBC (quiz 1) + JS : (30/09/2022) Create a Python Module, classes (__init__, __call__, etc…), operator overloading, files handling,\nJS : (03/10/2022 + 07/10/2022) unit tests\nJS : (10/10/2022 + 14/10/2022) Pandas: first steps / missing data\nJS : (17/10/2022 + 21/10/2022) scipy, numpy: Images/channel\nJS (quiz 2) : (28/10/2022) Sparse matrices, graphs and memory\nBC : (18/11/2022) Documentation with Sphinx\nJS + BC : (09/12/2022) The end: Project presentations\n\n\n\n\n\n\nShort quiz of 20 min each (on Moodle). This will be a personal work.\n\nQuiz 1 BC (30/09/2022, 10%)\nQuiz 2 JS (28/10/2022, 10%)\n\n\n\n\nWarning: the precise details of the projects might evolve before the allocation phase, and a precise grid will be given in the project section.\nWarning: the project repository must show a balanced contribution between group members and intra-group grades variation could be made to reflect issues on the intra-group workload balance.\n\n\n\n1 supplementary point on the final grade of the course can be obtained for contributions improving the course material (practicals, Readme, etc.). See the Bonus section for more details on how to proceed.\n\n\n\n\nThe resources for the course are available on the present github repository. Additional elementary elements (in French) on Python are available in the course HLMA310 and the associated lectures notes IntroPython.pdf.\n\n\n\n(General) : The Missing Semester of Your CS Education\n(Data Science) : J. Van DerPlas, Python Data Science Handbook, With Application to Understanding Data, 2016https://jakevdp.github.io/PythonDataScienceHandbook/\n(General) Skiena, The algorithm design manual, 1998\n(General) Courant et al. , Informatique pour tous en classes préparatoires aux grandes écoles : Manuel d’algorithmique et programmation structurée avec Python, 2013, (french)\n(General/data science) Guttag, Introduction to Computation and Programming, 2016\nAssociated videos: http://jakevdp.githubio/blog/2017/03/03/reproducible-data-analysis-in-jupyter/\n(Code and style) Boswell et Foucher, The Art of Readable Code, 2011\n(Scientific computing tools for Python) http://www.scipy-lectures.org/\n(Visualization) http://openclimatedata.net/"
  },
  {
    "objectID": "Courses/Bash/tp.html",
    "href": "Courses/Bash/tp.html",
    "title": "Introduction to bash",
    "section": "",
    "text": "The shell is the program that processes commands and returns output, e.g., Bash, zsh, etc…\nA terminal refers to a wrapper program that runs a shell.\nThe console is a special sort of terminal (low-level).\nReference: https://superuser.com/questions/144666/what-is-the-difference-between-shell-console-and-terminal\n\n\n\nAs you already know bash prompt is a $ sign when you are a standard user. When you are an administrator (often called root user) the prompt is a #."
  },
  {
    "objectID": "Courses/Bash/tp.html#the-unix-directory-structure",
    "href": "Courses/Bash/tp.html#the-unix-directory-structure",
    "title": "Introduction to bash",
    "section": "The Unix directory structure",
    "text": "The Unix directory structure\nMany details on the Unix directory structure at https://www.howtogeek.com/117435/htg-explains-the-linux-directory-structure-explained/\nSome aliases:\n\n~ is an alias to your home directory.\n. is an alias to the current directory.\n.. is an alias to the parent directory.\n\nFor instance\n$ cd ~\n$ pwd\n$ cd ../../home/../etc/../home/\n$ pwd\n\n\n\n\n\n\n### Exercise:\n\n\n1. What is the difference between cd ./toto/tata, cd toto/tata cd ~/toto/tata and cd /toto/tata 2. Use the which command to determine which instance of python is used when you use the python command. Same question with python2."
  },
  {
    "objectID": "Courses/Bash/tp.html#getting-help",
    "href": "Courses/Bash/tp.html#getting-help",
    "title": "Introduction to bash",
    "section": "Getting help",
    "text": "Getting help\nTo get some help for a command, please use the man command. You may also use the --help option as in\n$ man ls\n$ ls --help"
  },
  {
    "objectID": "Courses/Bash/tp.html#paging-programs",
    "href": "Courses/Bash/tp.html#paging-programs",
    "title": "Introduction to bash",
    "section": "Paging programs",
    "text": "Paging programs\nA paging program displays, one windowful at a time, the contents of a file on a terminal. It pauses after each windowful and prints on the window status line the screen the file name, current line number, and the percentage of the file so far displayed. This is not an editor (no modification of the file can be done)\nmore (deprecated) less (best choice) most (default on your machine, more feature than less, but bad keybindings).\n$ man less\n$ man most\nUseful tips:\n\nto search for a word type s. To go to the next (resp. previous) occurrence type n (resp. N).\n[less only] to go down type j, to go up type k.\nto go to the beginning of file type g, to the end G.\nTo quit type q.\nto change the default paging program to less.\n$ export MANPAGER=less\n\nMore resources: https://unix.stackexchange.com/questions/81129/what-are-the-differences-between-most-more-and-less"
  },
  {
    "objectID": "Courses/Bash/tp.html#pattern-matching-part-i-pathname-expansion-a.k.a.-globbing",
    "href": "Courses/Bash/tp.html#pattern-matching-part-i-pathname-expansion-a.k.a.-globbing",
    "title": "Introduction to bash",
    "section": "Pattern matching (part I): Pathname expansion (a.k.a. globbing)",
    "text": "Pattern matching (part I): Pathname expansion (a.k.a. globbing)\nIt is often very useful to select some files which filename contains (or not!) a specific pattern. Shells (bash, zsh, etc…) comes with a “pattern matching” syntax allowing us to express such constraints on the filenames.\nThis syntax is commonly called globs and is quite simple (more advance syntaxes called regexp will be introduced later on). globs are shell commands and can be transmitted to various program (ls, grep, find, etc…). For instance to display all the files with an extension in .txt in the current directory:\n$ ls *.txt\nMost shells have similar glob rules, and they usually consist of:\n\nA marker for zero-or-more characters: *\nA marker for exactly one character: ?\nA way to express one of a certain set of characters: [...]\nA way to express a choice of one or more strings: {...,...}\nA way to escape any of the above special characters: \\\n\n\n\n\n\n\n\n### Exercise:\n\n\n1. Go to /usr/lib/R/bin/ and list every file starting with a letter R and containing i 2. Go to /usr/lib/R/bin/ and list every file containing the letter c, then any character, and then a n (e.g. config or javareconf) 3. Got to /var/log/ and list every files with a double extension: the former one is a dot followed by a number, the last one is .log (e.g. Xorg.3.log or Xorg.0.log) 4. Got to /var/log/ and list every files with a name starting with a a and containing at least a digit"
  },
  {
    "objectID": "Courses/Bash/tp.html#listing-files",
    "href": "Courses/Bash/tp.html#listing-files",
    "title": "Introduction to bash",
    "section": "Listing files",
    "text": "Listing files\nTo list the files in a folder use the command ls.\n\n\n\n\n\n\n### Exercise:\n\n\n1. describe the option -a. 2. describe the option -R. 3. describe the option -lh. 4. List all the files in the directory /usr/lib/ without cd in it.\n\n\n\nThe file command can be used to display the information of a file (if not given by the extension itself).\n\n\n\n\n\n\n### Exercise:\n\n\n1. List all the files in the directory /usr/lib/R/bin and sort them by size. 2. Display the type information of the files in /var/log/ one call to file."
  },
  {
    "objectID": "Courses/Bash/tp.html#symbolic-links",
    "href": "Courses/Bash/tp.html#symbolic-links",
    "title": "Introduction to bash",
    "section": "Symbolic links",
    "text": "Symbolic links\nA symbolic link or symlink is a special file containing a reference (a link) to another file or directory. For instance try\n$ ls -l /usr/bin\nA symlink can be created with the command ln.\n$ ln -s target_path link_path\nObiously, replace target_path and link_path by their coresponding values.\n\n\n\n\n\n\n### Exercise:\n\n\n1. Create a symlink called my_etc_dir_link pointing to /etc in your home directory. 2. Then compare the output of ls /etc and ls ~/my_etc_dir_link"
  },
  {
    "objectID": "Courses/Bash/tp.html#users",
    "href": "Courses/Bash/tp.html#users",
    "title": "Introduction to bash",
    "section": "Users",
    "text": "Users\nTo list the groups you belong to, in a terminal use the command\n$ groups\nTo list the connected user on your machine\n$ w\n$ who"
  },
  {
    "objectID": "Courses/Bash/tp.html#file-permissions",
    "href": "Courses/Bash/tp.html#file-permissions",
    "title": "Introduction to bash",
    "section": "File permissions",
    "text": "File permissions\nEach file has an owner (a user) and a group (a group of users). To change the user that owns use chown and to change the group use chgrp. There are 3 types of permissions:\n\nread r\nwrite w\nexecute x\n\nThere are three permissions triads\n\nfirst triad: what the user can do (letter u)\nsecond triad: what the group members can do (letter g)\nthird triad: what other users can do (letter o)\n\nEach triad\n\nfirst character r: readable\nsecond character w: writable\nthird character x: executable\n\nTo change the permissions of file, use the chmod. For instance, to add execution x right to the owner u:\n$ chmod u+x toto.txt\n\n\n\n\n\n\n### Exercise:\n\n\n1. Create an empty file called foo.py in the current directory 2. Display its owner, group and permissions 3. Change the group of foo.py to pulse 4. Add read and write permissions to user in the group pulse\n\n\n\nRef: https://en.wikipedia.org/wiki/File_system_permissions. See also chown and chgrp"
  },
  {
    "objectID": "Courses/Bash/tp.html#environment-variables",
    "href": "Courses/Bash/tp.html#environment-variables",
    "title": "Introduction to bash",
    "section": "Environment variables",
    "text": "Environment variables\nAn environment variable (in short env or envs) is a dynamic-named value that can affect the way running processes will behave on a computer. Many options of bash may be change with envs. To print all the defined envs:\n$ printenv\nTo display a single variable, you may use the prefix $. For instance, to display the content of PATH\n$ echo ${PATH}\nTo set a new variable (in bash)\n$ export ENV_NAME=toto:tata\nLists are often separated by :. To append a new value at the end\n$ export ENV_NAME=${ENV_NAME}:tutu\n$ echo ${ENV_NAME}\nSome documentation: https://www.digitalocean.com/community/tutorials/how-to-read-and-set-environmental-and-shell-variables-on-a-linux-vps\n\n\n\n\n\n\n### Exercise:\n\n\n1. Display the PATH env 2. Is the order of the list important?\n\n\n\nUseful tips:\n\nTo avoid setting up an env every time you open a terminal, you can append the export MYENV=xxxxx command to the ~/.bashrc file.\n\n\nText editor\nIn bash, many configuration files are in fact text file. You may need to choose a text editor to modify them. Very powerful (and thus complicated) text editors exist: emacs, vim, but we will focus on nano (gedit is another alternative):\n$ nano\nor joe (default on your system).\n\n\n\n\n\n\n### Exercise:\n\n\n1. Set nano as your default text editor\n\n\n\n\n\nUseful unix commands\n\nlist files and get informations: ls, file, find\ndisplay text content: echo, cat, head, tail, grep, fgrep, rgrep\nfile handling: touch, mv, cp, rsync, rename\nunix admin: which, who, top, htop, kill, pkill, killall"
  },
  {
    "objectID": "Courses/Bash/tp.html#system",
    "href": "Courses/Bash/tp.html#system",
    "title": "Introduction to bash",
    "section": "System",
    "text": "System\n\nGetting system information\nTo display the system information\n$ uname -a\nTo show the system hostname you may use hostname command.\nTo show information about your processor use lscpu and to list the devices connected to your machine use lspci.\n\n\n\n\n\n\n### Exercise:\n\n\n1. Determine how many physical core you have on your machine. 2. Determine the vendor of the network card of your machine.\n\n\n\n\n\nProcess\nLearn how to use ps, top, htop, kill, pkill, … reading https://www.tutorialspoint.com/unix/unix-processes.htm\n\n\n\n\n\n\n### Exercise:\n\n\n1. Describe the effect of Ctrl+C in a terminal 2. Describe the effect of Ctrl+Z in a terminal 3. Describe the effect of Ctrl+D in a terminal"
  },
  {
    "objectID": "Courses/Bash/tp.html#display-text-content",
    "href": "Courses/Bash/tp.html#display-text-content",
    "title": "Introduction to bash",
    "section": "Display text content",
    "text": "Display text content\n\nGet the data\nThe dataset we are going to use is available at https://www.data.gouv.fr/fr/datasets/accidents-de-velo-en-france/. We will focus on bicycles accident in France from 2005-2017.\n\n\n\n### Exercise:\n\n\n1. Create a folder data_bicycle and cd to it. 2. Download the .csv file available at the following URL: https://koumoul.com/s/data-fair/api/v1/datasets/accidents-velos/raw as bicycle_db.csv (use the option -O of wget or redirect the output of curl with -o operator described below). Details (in French) on the dataset are available here: https://www.data.gouv.fr/fr/datasets/accidents-de-velo/\n\n\n\n\n\nText commands: tail, head, cat, wc and split\nPlease read the manual of tail, head, cat, wc and split\n\n\n\n\n\n\n### Exercise:\n\n\n1. Use the word count wc command to display the number of lines of bicycle_db.csv 2. Display the 53 first line with the head command. Same with the 30 last lines (see tail) 3. Use the split command and its options -d -l and --additional-suffix to create files with a maximum number of lines of 10000 (e.g. :if the number of lines is 55379, you should get only 6 files with names bike00.csv, …, bike05.csv)\n\n\n\n\n\nThe grep command\ngrep prints lines of a file matching a pattern (regex).\n$ man grep\n\n\n\n\n\n\n### Exercise:\n\n\n1. Count the number of accident in 2005 using the command grep (hint: remark that each line starts with the string \"YYYY where YYYY is the year) 2. Display the line number of the accident occurring on a Wednesday, in October 2017 using a regular expression.\n\n\n\n\n\nThe find command\nThe find command search for files in a directory hierarchy. Read the manual. For instance:\n$ find /usr/lib/ -name \"*qt5*\" -type f\nlist all the files in /usr/lib/ containing the qt5 string in its name.\n\n\n\n\n\n\n### Exercise:\n\n\n1. What is the aim of the -exec option? 2. Change the permissions of any file with extension .csv in your home to 777\n\n\n\nReference: https://www.tecmint.com/35-practical-examples-of-linux-find-command/\n\n\nThe args command\nTODO"
  },
  {
    "objectID": "Courses/Bash/tp.html#pipes-and-redirections",
    "href": "Courses/Bash/tp.html#pipes-and-redirections",
    "title": "Introduction to bash",
    "section": "Pipes and redirections",
    "text": "Pipes and redirections\n\n\n\nstream image\n\n\nThe I/O of any program launch through the bash are organized in three data streams:\n\nSTDIN (0): standard input (input)\nSTDOUT (1): standard output (data output by the command and printed in the terminal)\nSTDERR (2): standard error (reserved for error messages, also printed in the terminal)\n\nPiping and redirection is the process used to connect these streams between programs and files.\nSee: https://ryanstutorials.net/linuxtutorial/piping.php\n\nPipes\nIn bash the pipe operator is denoted |. It allows to compose (mathematically) the output of a program as an input of another one. For instance to display the 10 largest file given by du (disk use)\n$ du | sort -nr | head\nor display it in a pager\n$ du | sort -nr | less\n\n\n\n\n\n\n### Exercise:\n\n\n1. Display the last 15 accidents occurring with Vent fort condition 2. Display the type of crossing of the accident occurring with Vent fort in 2010. It should return\n\n\nbash Intersection en X Intersection en T Intersection en X\n\n\n\n\n\nRedirection\nThe operator > redirect the stdout of a command (LHS) into a file (RHS). Warning! it erases the file content. The operator >> append the output of the LHS to a file.\n$ ls /etc > toto.txt\n$ cat toto.txt\n$ wc -l toto.txt >> toto.txt\n$ cat toto.txt\nFinally, the operator < read from the file (RHS) and send the content to stdin (LHS)\n$ wc -l < toto.txt\n\n\n\n\n\n\n### Exercise:\n\n\n1. Create a single file bike2016.csv containing all the accident that occurred in 2016. 2. Append the accidents of year 2017 to the previous file and then rename it bike2016_17.csv."
  },
  {
    "objectID": "Courses/Bash/tp.html#pattern-matching-part-ii-regexp",
    "href": "Courses/Bash/tp.html#pattern-matching-part-ii-regexp",
    "title": "Introduction to bash",
    "section": "Pattern matching (part II): Regexp",
    "text": "Pattern matching (part II): Regexp\nA regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that define a search pattern. Many language implement such syntaxes (beware, there may be some differences!). Some of the most common regular expressions (share by almost all implementations) are\n\n\\ escape character\n^ start of line\n. any single character\n$ end of line\nx* zero or more occurrence of character x\nx+ one or more occurrences of character x\nx? zero or one occurrence of character x\nx{n} exactly n occurrence of character x\n[...] range of characters (e.g. [a-z], [A-Z], [a-zA-Z], [0-9], etc…)\n[^...] forbidden characters range\n(...) marked subexpression. The string matched within the parentheses can be recalled later (see the next entry, ). A marked subexpression is also called a block or capturing group. …\n\nFor instance, to capture all the word starting with a capital letter in a text, you may use the regexp:\n([A-Z][a-zA-Z0-9_]*)+\nSee https://regexr.com/. See also the doc of the sed, awk programs and the perl language. Reference: https://en.wikipedia.org/wiki/Regular_expression\n\n\n\n### Exercise:\n\n\n1. Go to https://regex101.com/ and copy/paste the following list (in the TEST STRING frame):\n\n\n3. Capture with a regexp all the song names (between the track number and the extension). You should get this in the MATCH INFORMATION frame on the right:"
  },
  {
    "objectID": "Courses/CI/tp.html",
    "href": "Courses/CI/tp.html",
    "title": "Continuous Integration (CI)",
    "section": "",
    "text": "In software engineering, continuous integration (CI) is the practice of merging all developers’ working copies to a shared mainline on a regular basis. It is often split in 3 steps:\n\nautomate the tests: run command on each commit (or each Pull Request), typically unit tests and integration tests.\nautomate the build: when dealing with a compiled language, compile the source to generate binaries. I can also build the documentation.\nautomate the deployment: send the binaries to the repository.\n\nA CI pipeline runs commands on some virtual machine automatically.\nReference: https://help.github.com/en/actions/building-and-testing-code-with-continuous-integration/setting-up-continuous-integration-using-github-actions\n\nBenefits of CI\n\nOne can not forget to run tests and immediate feedback is provided: it runs at each commit or Pull Request. A report is sent to the commit author.\nProtects the master branch: commit or PR can be rejected if test do not pass.\nContributor doesn’t need to know details: only project maintainer needs to know how the system works.\nCan enforce style: a linter can run to check PEP8.\nCan check the code on many systems: virtual machines can run Linux, Window or MacOs systems.\n\n\n\nWhat do you need ?\nMany solutions exist to run CI pipelines (Gitlab, Github, Jenkins, TravisCI, Appveyor, Azure Pipelines, CircleCI…). They all:\n\nrun test when a web-hook is triggered (usually at each push or PR).\ncan act as a build-farm (for binaries or documentation) on a “build matrix” (i.e. run on many environments).\nRequires clear declaration of dependencies and set-up virtual machines (that should be maintained).\nReports success/Failure to the CSV.\n\n\n\nExample\nGithub has recently developed a high-level solution of CI. Before digging into the process, please make sure that your test file is working locally. You should have something like:\n$ pytest\n============================= test session starts ==============================\nplatform linux -- Python 3.7.6, pytest-5.3.5, py-1.8.1, pluggy-0.13.1\nrootdir: /home/bcharlier/packaging_tutorial\nplugins: cov-2.8.1\ncollected 3 items\n\nbiketrauma/tests/test_biketrauma.py ...                                  [100%]\n\n=============================== warnings summary ===============================\n/home/bcharlier/.local/lib/python3.7/site-packages/pygal/_compat.py:23\n  /home/bcharlier/.local/lib/python3.7/site-packages/pygal/_compat.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n    from collections import Iterable\n\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\n========================= 3 passed, 1 warning in 1.56s =========================\n\nAdd a .github/workflows file\nSetting up a CI is rather easy. It is sufficient to add a single text file .github/workflows in your project. Github has developed a graphical user interface to do it:\n\nIn your github project repository: Go to Actions menu and then select python package workflow.\nCustom the workflows file depending on your needs. Beware: getting a correct configuration file is sometime tedious with CI system…\nYou can add a badge showing the result of CI to the end-user directly in your Readme.md\n\n\n\n\n### Exercise:\n\n\n1. Fork the biketrauma project https://github.com/HMMA238-2020/biketrauma 2. Setup a CI with github on biketrauma"
  },
  {
    "objectID": "Courses/Docs/tp.html",
    "href": "Courses/Docs/tp.html",
    "title": "Documentation with Sphinx",
    "section": "",
    "text": "Sphinx is an extension of reStructuredText. reStructuredText (.RST, .ReST, or .reST) is a file format for textual data used primarily in the Python programming language community for technical documentation, and similar to the Markdown format (.md) you are currently reading.\nIt is part of the Docutils project of the Python Doc-SIG (Documentation Special Interest Group), aimed at creating a set of tools for Python similar to Javadoc for Java or Plain Old Documentation (pod) for Perl or vignette for R.\nDocutils can extract comments and information from Python programs, and format them into various forms of program documentation.\nIn this sense, reStructuredText is a lightweight markup language designed to be both:\n\nprocessable by documentation-processing software such as Docutils,\neasily readable by human programmers who are reading and writing Python source code.\n\nReference: https://en.wikipedia.org/wiki/ReStructuredText, https://en.wikipedia.org/wiki/Comparison_of_document-markup_languages and https://en.wikipedia.org/wiki/Comparison_of_documentation_generators\n\n\n\nA ReST file is a plain text file with a .rst extension. Alike Markdown (.md) it allows you to easily write formatted text.\n\n\nSection Header\n==============\n\nSubsection Header\n-----------------\n\n\n\n- A bullet list item\n- Second item\n\n  - A sub item (indentation matters!)\n\n- Spacing between items creates separate lists\n\n- Third item\n\n1) An enumerated list item\n\n2) Second item\n\n   a) Sub item that goes on at length and thus needs\n      to be wrapped. Note the indentation that must\n      match the beginning of the text, not the\n      enumerator.\n\n      i) List items can even include\n\n         paragraph breaks.\n\n3) Third item\n\n#) Another enumerated list item\n\n#) Second item\n\n\n\n\n.. image:: /path/to/image.jpg\n   :height: 100\n   :width: 200\n   :scale: 50\n   :align: center\n   :alt: ordinateur\n\n   Caption text rendered below the image...\n\n\n\nA sentence with links to `Wikipedia`_ and the `Linux kernel archive`_.\n.. _Wikipedia: https://www.wikipedia.org/\n.. _Linux kernel archive: https://www.kernel.org/\nAnother sentence with an `anonymous link to the Python website`__.\n__ https://www.python.org/\nN.B.: named links and anonymous links are enclosed in grave accents (`), and not in apostrophes (’).\nN.B.: it is possible to create references to label linked to an image, a section, in the .rst file etc…\n\n\n\n::\n\n  some literal text\n\nThis may also be used inline at the end of a paragraph, like so::\nsome more literal text\n.. code:: python\n\n   print(\"A literal block directive explicitly marked as python code\")"
  },
  {
    "objectID": "Courses/Docs/tp.html#set-up-the-doc",
    "href": "Courses/Docs/tp.html#set-up-the-doc",
    "title": "Documentation with Sphinx",
    "section": "Set up the doc",
    "text": "Set up the doc\nReference: this part is mainly from the Sphinx documentation http://www.sphinx-doc.org/en/stable/.\nThe documentation is usually located in a docs or doc folder located at the root of a project. For instance in the biketrauma module we have:\npackaging_tutorial/\n    biketrauma/\n        __init__.py\n        data/\n        vis/\n        io\n        tests/\n    doc/\n    setup.py\n    .gitignore\nIn the Sphinx terminology, this doc folder is called the source directory. It contains:\n\na configuration file conf.py with all the information needed to read the sources and build the doc. By building, it is meant the process of generating the doc (usually in html, pdf, etc.) from the ReST files.\na directory structure containing .md or .rst files with the doc.\n\nTo help you, Sphinx comes with a script called sphinx-quickstart that sets up a source directory and creates a default conf.py with the most useful configuration values from a few questions it asks you. To use this, run:\n$ sphinx-quickstart\nAnswer each question asked. Be sure to say yes to the autodoc extension, as we will use this later. There is also an automatic API documentation (API: Application Programming Interface) generator called sphinx-apidoc; see sphinx-apidoc for details.\n\n\n\n### Exercise:\n\n\nSet up the documentation for biketrauma python module.\n\n\n1. Install the sphinx package with pip 2. Create a doc folder and cd into it 3. Launch sphinx-quickstart --sep."
  },
  {
    "objectID": "Courses/Docs/tp.html#defining-documentation-structure",
    "href": "Courses/Docs/tp.html#defining-documentation-structure",
    "title": "Documentation with Sphinx",
    "section": "Defining documentation structure",
    "text": "Defining documentation structure\nLet us assume you have run sphinx-quickstart. It has created a source directory with conf.py and a master document, index.rst (if you accepted the defaults parameters).\nThe main function of the master document is to serve as a welcome page, and to contain the root of the “table of contents tree” (or toctree). This is one of the main things that Sphinx adds to reStructuredText, a way to connect multiple files to a single hierarchy of documents.\nThe toctree directive initially is empty, and looks like so:\n.. toctree::\n   :maxdepth: 2\nYou add documents listing them in the content of the directive:\n.. toctree::\n   :maxdepth: 2\n\n   usage/installation\n   usage/quickstart\n   ...\nThis is exactly how the toctree for this documentation looks. The documents to include are given as document names, which in short means that you leave off the file name extension and use forward slashes (/) as directory separators.\n\n\n\n### Exercise:\n\n\n\n\n## Building the doc\n\n\nDuring the configuration of Sphinx, a text file called MakeFile has been created: in software development, Make is a build automation tool that automatically builds executable programs and libraries from source code by reading files called Makefiles which specify how to derive the target program.\n\n\nReference: https://en.wikipedia.org/wiki/Make_(software)\n\n\nbash $ make html\n\n\nThen to access the web pages created:\n\n\nbash $ firefox _build/html/index.html\n\n\nN.B.: there is also a sphinx-build tool that can help you to build without make.\n\n\n\n\nExercise:\n\nList all the target defined in the Makefiles\nBuild your doc and visualize it with a navigator"
  },
  {
    "objectID": "Courses/Docs/tp.html#api-doc-autodoc",
    "href": "Courses/Docs/tp.html#api-doc-autodoc",
    "title": "Documentation with Sphinx",
    "section": "API doc (autodoc)",
    "text": "API doc (autodoc)\nWhen documenting Python code, it is common to put a lot of documentation in the source files, in documentation strings. Sphinx supports the inclusion of docstrings from your modules with an extension (an extension is a Python module that provides additional features for Sphinx projects) called autodoc.\nIn order to use autodoc, you need to activate it in conf.py by putting the string 'sphinx.ext.autodoc' into the list assigned to the extensions config value. Then, you have a few additional directives at your disposal.\nFor example, to document the function io.open(), reading its signature and docstring from the source file, you’d write this:\n.. autofunction:: io.open\nYou can also document whole classes or even modules automatically, using member options for the auto directives, like\n.. automodule:: io\n   :members:\nautodoc needs to import your modules in order to extract the docstrings. Therefore, you must add the appropriate path to sys.path in your conf.py.\n\n\n\n\n\n\n### Exercise:\n\n\n1. Write a docstring for the class biketrauma.io.Load_db and the function plot_location 2. Integrate these documentations in a section called API in the sphinx toctree."
  },
  {
    "objectID": "Courses/Docs/tp.html#sphinx-gallery",
    "href": "Courses/Docs/tp.html#sphinx-gallery",
    "title": "Documentation with Sphinx",
    "section": "Sphinx-Gallery",
    "text": "Sphinx-Gallery\nSphinx-Gallery is an extension able to create galleries of example in the html documentation directly from the scripts files of your project. See e.g., https://sphinx-gallery.github.io/stable/auto_examples/index.html\n\nConfiguration\nConfiguration and customization of sphinx-gallery is done primarily with a dictionary specified in your conf.py file. A typical sample\nfrom sphinx_gallery.sorting import FileNameSortKey\nsphinx_gallery_conf = {\n     # path to your examples scripts\n    'examples_dirs': ['../script',],\n     # path where to save gallery generated examples\n    'gallery_dirs': ['_auto_scripts'],\n    # order of the Gallery\n    'within_subsection_order': FileNameSortKey,\n}\nA list of the possible keys can be found here https://sphinx-gallery.github.io/stable/configuration.html.\n\n\n\n\n\n\n### Exercise:\n\n\n1. Install the sphinx-gallery extension with pip. 2. Update the conf.py of the biketrauma package with the dictionary containing the configuration of the sphinx-gallery.\n\n\n\n\n\nStructure your examples files\nSphinx-Gallery parse the folder listed in the key examples_dirs. It expects each Python file to have two things:\n\nA docstring, written in rST, that defines the header for the example. It must begin by defining a rST title. The title may contain any punctuation mark but cannot start with the same punctuation mark repeated more than 3 times. For example:\n\n    \"\"\"\n    \"This\" is my example-script\n    ===========================\n\n    This example doesn't do much, it just makes a simple plot\n    \"\"\"\n\nPython code. This can be any valid Python code that you wish. Any matplotlib images that are generated will be saved to disk, and the rST generated will display these images with the built examples. By default only images generated by matplotlib, or packages based on matplotlib (e.g., seaborn or yellowbrick) are saved and displayed. However, you can change this to include other packages, see for instance Image scrapers (XXX TODO: add details on Image scrapers).\n\nWarning: with default options, Sphinx-Gallery only execute the script files with a filename starting by plot_.\nWarning: Sphinx-Gallery expect finding a README.txt (or README.rst) file in every folders containing examples.\n\n\nInclude examples in your toctree\nFor instance you can add those lines in the index.rst\n.. toctree::\n   :maxdepth: 2\n   :caption: Previsions:\n   \n   _auto_scripts/index\nto add a section containing all the examples.\n\n\n\n\n\n\n### Exercise:\n\n\n1. Transform the script.py examples into an auto build example."
  },
  {
    "objectID": "Courses/Git/tp.html",
    "href": "Courses/Git/tp.html",
    "title": "Setting up your local git identity",
    "section": "",
    "text": "On a terminal, specify the email address with which you will make your commits:\n$ git config --global user.email \\\"prenom.nom@domaine.fr\\\"\nAdapt the email address prenom.nom@domaine.fr to your case!"
  },
  {
    "objectID": "Courses/Git/tp.html#create-an-ssh-key",
    "href": "Courses/Git/tp.html#create-an-ssh-key",
    "title": "Setting up your local git identity",
    "section": "Create an SSH key",
    "text": "Create an SSH key\n\nUnix system\nThe SSH is needed to get a smooth authentication to the remote repository. In a terminal:\n$ ssh-keygen -t rsa -b 4096 -C prenom.nom@domaine.fr\nAccept the default option (keys saved in ~/.ssh and no passphrase)\nssh-add\nSee the following link for more details: https://help.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\n\nWindows\nPlease see the tutorial at https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/GitHub-SSH-Windows-Example"
  },
  {
    "objectID": "Courses/Git/tp.html#create-a-remote-repository",
    "href": "Courses/Git/tp.html#create-a-remote-repository",
    "title": "Setting up your local git identity",
    "section": "Create a remote repository",
    "text": "Create a remote repository\nLet us create a remote repository hosted on your GitHub account.\nOn GitHub, click on the + symbol at the top right of the page, then New repository. Give the name FirstRepo to your new project and a short description.\nCreate a public repository, meaning that everyone can access to your code (read only). Finish by clicking on Create repository.\nFollow the instructions provided by GitHub to create your local copy of the repository: 1. Create a new folder called FirstRepo in your home directory and cd to it 2. Then execute the following command\necho \"# FirstRepo\" >> README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:XXXXXXXXXXXXXXXXXX/FirstRepo.git\ngit push -u origin master\n\n Exercise \n\nCreate a text file called .gitignore with the following content:\n\n*.pdf\n*~\n\nCreate a commit and push it to your repository. What is the purpose of this file? See https://github.com/github/gitignore"
  },
  {
    "objectID": "Courses/Git/tp.html#using-an-existing-repository",
    "href": "Courses/Git/tp.html#using-an-existing-repository",
    "title": "Setting up your local git identity",
    "section": "Using an existing repository",
    "text": "Using an existing repository\nBrowse the repository at https://github.com/bcharlier/HAX712X-2022. What is this module able to do?\n\nExercise\nFork the repository by following these steps:\n\nOn GitHub, click on the fork icon.\nA copy is added to your GitHub space. Clone it (this copy!) to get a local repository.\nIn a terminal, inspect the output of the command git remote get-url origin"
  },
  {
    "objectID": "Courses/Git/tp.html#debugging",
    "href": "Courses/Git/tp.html#debugging",
    "title": "Setting up your local git identity",
    "section": "Debugging",
    "text": "Debugging\nA bug has appeared into the python module after some commit. An issue has been opened in the bug tracking system at https://github.com/bcharlier/HAX712-2022/issues. Your goal is to find the problem… and then to fix it on your forked repository. Finally, you will be able to submit a Pull Request to the original repository to share your fix.\n\nIdentification of the bad commit\nYour goal is to identify the commit(s) that caused the bug. Use git log, git diff, git checkout to identify the commit responsible for the problem.\n\n\nCreate a new branch to fix the problem\nIn order to fix a complex bug or add a new feature, it is often necessary to modify several parts of the code. We create a branch, where we make all the commits dedicated to solve the bug. The idea is to maintain a stable version, in the branch master, separated from the developing version, which may contain bugs.\n\n\n Exercise\n\nCreate a local branch Fix_EOL_Error\nPush this local branch to your remote repo.\nCheckout to the Fix_EOL_Error branch, fix the bugs. The branch master will not be affected.\nMerge the fix into the branch master\nsuppress the local branch Fix_EOL_Error and the remote origin/Fix_EOL_Error branch\n\n\n\nPull request\nYour work about bug fixing may interest the original author of the project. On GitHub, open a pull-request. Pull-requests are a set of commits which can be integrated directly by the author of the project in its repository, and are thus a powerful tool for working with others."
  },
  {
    "objectID": "Courses/Git/tp.html#branch-merging-and-solving-conflict",
    "href": "Courses/Git/tp.html#branch-merging-and-solving-conflict",
    "title": "Setting up your local git identity",
    "section": "Branch Merging and Solving conflict",
    "text": "Branch Merging and Solving conflict\n\nExercise\n\nCheckout to the branch NonGaussian. Try to figure out what has changed compared to the master branch.\nTry to merge the branch NonGaussian to the branch master.\nWhere are located the conflicts? They are shown with the following decorator.\n\n<<<<<<< HEAD\nsome code on current branch\n=======\nsome code on branch to be merged\n>>>>>>> NonGaussian\n\nResolve them by plotting the two histograms on the same plot. Namely, produce a figure like this:"
  },
  {
    "objectID": "Courses/IDE/tp.html",
    "href": "Courses/IDE/tp.html",
    "title": "Integrated development environment",
    "section": "",
    "text": "An Integrated Development Environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools and a debugger."
  },
  {
    "objectID": "Courses/IDE/tp.html#pythons-specific-ide",
    "href": "Courses/IDE/tp.html#pythons-specific-ide",
    "title": "Integrated development environment",
    "section": "python’s specific IDE",
    "text": "python’s specific IDE\nThere are many IDE for Python, but none are perfect, and there is no consensus in the python community. There is no real “canonical” choice as Rstudio is the one for R user.\nAs python is a real jackknife programming language, depending on your goal (data scientific program, web development, etc.) you may choose a specific IDE for a particular task.\n\nScientific computing : Pyzo, Spyder\nGeneric: PyCharm, VSCode\n\nWe warmly recommend you to use an IDE, and we will mostly describe VSCode in what follows."
  },
  {
    "objectID": "Courses/IDE/tp.html#vscodecodium",
    "href": "Courses/IDE/tp.html#vscodecodium",
    "title": "Integrated development environment",
    "section": "VSCode/Codium",
    "text": "VSCode/Codium\nFor instance you can use VSCode. This is a powerful, cross-platform IDE that comes with many extensions.\nOn the FdS-Linux box, there is a fork of vscode called vscodium. You may launch it via the GUI or through the following command line\n$ vscodium\n\nInstall a VSCode extension\nWe will install the python extension. To install it:\n\nOpen VSCode.\nOpen the Extensions tab (left bar of the VSCode window or simply press Ctrl+Shift+X)\nType python to find the python extension from Microsoft\nClick the Install button, then the Enable button\n\nor\n\nOpen VSCode\nPress Ctrl+P to open the Quick Open dialog\nType ext install ms-python.python to find the extension\nClick the Install button, then the Enable button\n\nor\n\nRun in a terminal\n\n$ vscodium --install-extension ms-python.python\n\n\n\n\n\n\n### Exercise:\n\n\n1. Install the python extension in your Vscode\n\n\n\n\n\nAn advance text editor\nThe keyboard shortcuts Reference guide is available in the help menu (or with Ctrl+K Ctrl+R shortcut). It can be very useful to learn some shortcuts. For instance:\n\nLearn how multicursors work (e.g., search for an occurrence with Ctrl+d)\nCreate aligned multicursors with Ctrl+Shift\nLearn how to move an entire line with Alt+up\netc.\n\n\n\nUsing VSCode as a python IDE\nThis part is from the tutorial https://code.visualstudio.com/docs/python/python-tutorial to set up vsCode to use it as a python IDE. You should have a working vscode (with python extension) and anaconda program.\n\n\n\n### Exercise:\n\n\n1. Start VS Code in a project (workspace) folder: Using a command prompt or terminal, create an empty folder called hmma238_test_dir, navigate into it, and open VS Code (code) in that folder (.) by entering the following commands:\n\n\nbash cd ~ mkdir hmma238_test_dir cd hello code . Note: If you’re using an Anaconda distribution, be sure to use an Anaconda command prompt.\n\n\nBy starting VS Code in a folder, that folder becomes your “workspace”. VS Code stores settings that are specific to that workspace in .vscode/settings.json, which are separate from user settings that are stored globally.\n\n\nAlternately, you can run VS Code through the operating system UI, then use File > Open Folder to open the project folder.\n\n\n2. Select a Python interpreter: Python is an interpreted language, and in order to run Python code, you must tell VS Code which interpreter to use.\n\n\nFrom within VS Code, select a Python 3 interpreter by opening the Command Palette (Ctrl+Shift+P), start typing the Python: Select Interpreter command to search, then select the command. You can also use the Select Python Environment option on the Status Bar if available\n\n\n3. Open the terminal in VS Code and download with wget or curl the file test_python.py at https://raw.githubusercontent.com/bcharlier/HMMA238/master/Courses/IDE/test_python.py. Run it through the IDE.\n\n\n4. Next, you have to learn how to debug a simple python script https://code.visualstudio.com/docs/python/debugging."
  },
  {
    "objectID": "Courses/IDE/tp.html#recommended-extensions-for-vs-code",
    "href": "Courses/IDE/tp.html#recommended-extensions-for-vs-code",
    "title": "Integrated development environment",
    "section": "Recommended extensions for VS Code",
    "text": "Recommended extensions for VS Code\n\nMarkdown visualizer (extension with TeX rendering even better : Markdown+Math)\nlinter/flake8: cornflakes\nSpell check for less typos: SpellChecker\nLaTeX compiler: XXX TODO\nLive Share to collaboratively edit and debug with others in real time, regardless of your programming language."
  },
  {
    "objectID": "Courses/Pandas/tp.html",
    "href": "Courses/Pandas/tp.html",
    "title": "Pandas",
    "section": "",
    "text": "This lecture is extracted and adapted from the work by Joris Van den Bossche https://github.com/jorisvandenbossche/pandas-tutorial/blob/master/01-pandas_introduction.ipynb\nFor R users, you might also want to read https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html for a smooth start.\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# widget manipulation\nfrom ipywidgets import widgets, interact, interactive, fixed, interact_manual\n\nimport pooch  # download data / avoid re-downloading\nfrom IPython import get_ipython\n\npd.options.display.max_rows = 8\n\n\nFirst, it is important to download automatically remote files for reproducibility (and avoid typing names manually)\nurl = \"http://josephsalmon.eu/enseignement/datasets/titanic.csv\"\npath_target = \"./titanic.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)  # if needed `pip install pooch`\nReading the file as a pandas dataframe:\ndf_titanic_raw = pd.read_csv(\"titanic.csv\")\nVisualize the end of the dataset:\ndf_titanic_raw.tail(n=3)\nVisualize the beginning of the dataset:\ndf_titanic_raw.head(n=5)\n\n\n\n\nIt is common to encounter features/covariates with missing values. In pandas they were mostly handled as np.nan (not a number). In the future, they will be treated as NA (note available), in a similar way as in R; see for standard behavior and details https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html.\nNote that the main difference between pd.NA and np.nan is that pd.NA propagates even for comparisons:\npd.NA == 1\n>>><A>\nwhereas\nnp.nan == 1\n>>>False\nTesting the presence of missing values\npd.isna(pd.NA)\n>>> True\npd.isna(np.nan)\n>>> True\nThe simplest strategy (when you can / when you have enough samples) consists in removing all nans/NAs.\ndf_titanic = df_titanic_raw.dropna()\ndf_titanic.tail(3)\n# Useful info on the dataset (especially missing values!)\ndf_titanic.info()\n# Check that cabin is mostly missing, also the age\ndf_titanic_raw.info()\n\n\nMore details can be found here: https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3info.txt\n\nSurvived: Survival 0 = No, 1 = Yes\nPclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\nSex: Sex male/female\nAge: Age in years\nSibsp: # of siblings / spouses aboard the Titanic\nParch: # of parents / children aboard the Titanic\nTicket: Ticket number\nFare: Passenger fare\nCabin: Cabin number\nEmbarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\nName: Name of the passenger\nPassengerId: Number to identify passenger\n\nNote: an extended version of the dataset is available here for those interested https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic.txt.\n\n\n\n\ndf_titanic.describe()\n\n\n\n\nHistograms (please avoid…often useless) python     plt.figure(figsize=(5, 5))     plt.hist(df_titanic['Age'], density=True, bins=25)     plt.xlabel('Age')     plt.ylabel('Proportion')     plt.title(\"Passager age histogram\")\nKernel Density Estimate (KDE): :\nplt.figure(figsize=(5, 5), num='jfpwje')\n# KDE: kernel density estimate\nax = sns.kdeplot(df_titanic['Age'], shade=True, cut=0, bw=0.1)  # bw: bandwith\nplt.xlabel('Proportion')\nplt.ylabel('Age')\nax.legend().set_visible(False)\nplt.title(\"Passager age kernel density estimate\")\nplt.tight_layout()\nSwarmplot: python     plt.figure(figsize=(10, 5), num='jfpwje')     # Swarmplot from seaborn package     ax = sns.swarmplot(data=df_titanic_raw, x='Sex', y='Age', hue='Survived',                     palette={0: 'r', 1: 'k'}, order=['female', 'male'])     plt.title(\"Passager age by gender/survival\")     plt.legend()     plt.tight_layout()\n\n\n\n\nPlot the density estimate over the histogram\n\n\n\nInteractive interaction with codes and output is nowdays easier and easier (see also Shiny app in R-software). In python one can use for that widgets and the interact package. We are going to visualize that on the simple KDE and histograms examples.\ndef hist_explore(\n    dataset=df_titanic,\n    variable=df_titanic.columns,\n    n_bins=24,\n    alpha=0.25,\n    density=False,\n):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    ax.hist(\n        dataset[variable], density=density, bins=n_bins, alpha=alpha\n    )  # standardization\n    plt.ylabel(\"Density level\")\n    plt.title(f\"Dataset {dataset.attrs['name']}:\\n Histogram for passengers' age\")\n    plt.tight_layout()\n    plt.show()\n\n\ninteract(\n    hist_explore,\n    dataset=fixed(df_titanic),\n    n_bins=(1, 50, 1),\n    alpha=(0, 1, 0.1),\n    density=False,\n)\ndef kde_explore(dataset=df_titanic, variable=df_titanic.columns, bw=5):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    sns.kdeplot(dataset[variable], bw_adjust=bw, shade=True, cut=0, ax=ax)\n    plt.ylabel(\"Density level\")\n    plt.title(f\"Dataset {dataset.attrs['name']}:\\n KDE for passengers'  {variable}\")\n    plt.tight_layout()\n    plt.show()\n\ninteract(kde_explore, dataset=fixed(df_titanic), bw=(0.001, 2, 0.01))\n\n\n\nHow does the survival rate change w.r.t. to sex?\ndf_titanic_raw.groupby('Sex')[['Survived']].aggregate(lambda x: x.mean())\nHow does the survival rate change w.r.t. the class?\ndf_titanic.columns\nplt.figure()\ndf_titanic.groupby('Pclass')['Survived'].aggregate(lambda x:\n                                                   x.mean()).plot(kind='bar')\nplt.xlabel('Classe')\nplt.ylabel('Taux de survie')\nplt.title('Taux de survie par classe')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.catplot(x='Pclass', y=\"Age\",\n            hue=\"Sex\", data=df_titanic_raw, kind=\"swarm\", legend=False)\nplt.title(\"Age par classe\")\nplt.legend(loc=1)\nplt.tight_layout()\nBeware: large difference in sex ratio by class\ndf_titanic_raw.groupby(['Sex', 'Pclass'])[['Sex']].count()\ndf_titanic_raw.groupby(['Sex'])[['Sex']].count()\nMore on groupby pandas-kungfu: cf. also pd.crosstab, etc. https://pbpython.com/groupby-agg.html\n\n\npd.crosstab(df_titanic_raw['Sex'],\n            df_titanic_raw['Pclass'],\n            values=df_titanic_raw['Sex'],\n            aggfunc='count',\n            normalize=False)\ndf_titanic\ndf_titanic.index\ndf_titanic.columns\npd.options.display.max_rows = 12\ndf_titanic.dtypes\n\ndf_titanic['Name'].astype(str)\n\n\n\n\n\nuseful for using packages on top of pandas (e.g., sklearn, though nowadays it works out of the box with pandas)\narray_titanic = df_titanic.values  # associated numpy array\narray_titanic\n\n\nPerform the following operation: remove the columns Cabin from the raw dataset, and then remove the rows with missing age. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n# XXX TODO\n\n\n\n\nA Series is a labelled 1D column of a kind.\nfare = df_titanic['Fare']\nfare\n\n\nfare.values[:10]\nContrarily to numpy arrays, you can index with other thing that integers:\ndf_titanic_raw = df_titanic_raw.set_index('Name')  # you can only do it once !!\ndf_titanic_raw\nage = df_titanic_raw['Age']\nage['Behr, Mr. Karl Howell']\nage.mean()\ndf_titanic_raw[age < 2]\ndf_titanic_raw = df_titanic_raw.reset_index()  # come back to original index\n\n\n\n\ndf_titanic_raw['Embarked'].value_counts(normalize=False, sort=True,\n                                        ascending=False)\npd.options.display.max_rows = 70\ndf_titanic[df_titanic['Embarked'] == 'C']\nComments: not all passagers from Cherbourg are Gaulois…\npd.options.display.max_rows = 8\ndf_titanic_raw['Survived'].sum() / df_titanic_raw['Survived'].count()\ndf_titanic['Survived'].mean()\nQ: What was the proportion of women on the boat?\n\n\n\ndf_titanic_raw.groupby(['Sex']).mean()\n\n\n\nPandas supports many formats: - CSV, text - SQL database - Excel - HDF5 - json - html - pickle - sas, stata - …\npd.read_csv?\n# http://josephsalmon.eu/enseignement/datasets/babies23.data\npd.read_csv('babies23.data', skiprows=38, sep='\\s+')\n# pd.read_csv?\n\n\n\ndf_titanic_raw.tail()\ndf_titanic_raw.head()\n\n\n\nsns.set_palette(\"colorblind\")\nsns.catplot(x='Pclass', y='Age', hue='Survived', data=df_titanic_raw,\n            kind=\"violin\")\n\n\n\n\niloc\n\ndf_titanic_raw.iloc[0:2, 1:8]\n\nloc\n\n# with original index:\n# df_titanic_raw.loc[128]\n\n# with naming indexing\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Fare']\n\n\n```python\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth']\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived']\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived'] = 0\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth']\n# set back the original value\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived'] = 1\n\n\ndf_titanic.groupby('Sex').mean()\ndf_titanic_raw.groupby('Sex').mean()['Pclass']\n\n\n\ndf_titanic['AgeClass'] = pd.cut(df_titanic['Age'], bins=np.arange(0, 90, 10))\ndf_titanic['AgeClass']\n\n\n\n\nurl = \"http://josephsalmon.eu/enseignement/datasets/20080421_20160927-PA13_auto.csv\"\npath_target = \"./20080421_20160927-PA13_auto.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n!head -26 ./20080421_20160927-PA13_auto.csv\nAlternatively :\nget_ipython().system('head -26 ./20080421_20160927-PA13_auto.csv')\n\n\nhttps://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html\npolution_df = pd.read_csv('20080421_20160927-PA13_auto.csv', sep=';',\n                          comment='#',\n                          na_values=\"n/d\",\n                          converters={'heure': str})\npd.options.display.max_rows = 30\npolution_df.head(25)\n\n\n\n\n\n\nWhat is the meaning of “na_values=”n/d” above? Note that an alternative can be obtained with the command polution_df.replace('n/d', np.nan, inplace=True)\n# check types\npolution_df.dtypes\n\n# check all\npolution_df.info()\nFor more info on the object nature (inherited from numpy), see https://stackoverflow.com/questions/21018654/strings-in-a-dataframe-but-dtype-is-object\n\n\n\nStart by changing to integer type (e.g., int8):\npolution_df['heure'] = polution_df['heure'].astype(np.int8)\npolution_df['heure']\nNo data is from 1 to 24… not conventional so let’s make it from 0 to 23\npolution_df['heure'] = polution_df['heure'] - 1\npolution_df['heure']\nand back to strings:\npolution_df['heure'] = polution_df['heure'].astype('str')\npolution_df['heure']\n\n\nhttps://www.tutorialspoint.com/python/time_strptime.htm\ntime_improved = pd.to_datetime(polution_df['date'] +\n                               ' ' + polution_df['heure'] + ':00',\n                               format='%d/%m/%Y %H:%M')\n\n# Where d = day, m=month, Y=year, H=hour, M=minutes\ntime_improved\npolution_df['date'] + ' ' + polution_df['heure'] + ':00'\nCreate correct timing format in the dataframe\npolution_df['DateTime'] = time_improved\n\ndel polution_df['heure']  # remove useless column\ndel polution_df['date']  # remove useless column\npolution_df\nVisualize the data set now that the time is well formated:\npolution_ts = polution_df.set_index(['DateTime'])\npolution_ts = polution_ts.sort_index(ascending=True)\npolution_ts.head(12)\npolution_ts.describe()\n# raw version\n# sns.color_palette(\"colorblind\")\n# cmap = sns.light_palette(\"Navy\", as_cmap=True)\nfig, axes = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\naxes[0].plot(polution_ts['O3'])\naxes[0].set_title(\"Ozone polution: daily average in Paris\")\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\n\naxes[1].plot(polution_ts['NO2'])\naxes[1].set_title(\"Nitrogen polution: daily average in Paris\")\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\nplt.show()\nfig, axes = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n\naxes[0].plot(polution_ts['O3'].resample('d').max(), '--')\naxes[0].plot(polution_ts['O3'].resample('d').min(),'-.')\n\naxes[0].set_title(\"Ozone polution: daily average in Paris\")\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\n\naxes[1].plot(polution_ts['NO2'].resample('d').max(),  '--')\naxes[1].plot(polution_ts['NO2'].resample('d').min(),  '-.')\n\naxes[1].set_title(\"Nitrogen polution: daily average in Paris\")\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\n\nplt.show()\n\n\n\nProvide the same plots as before, but with daily best and worst on the same figures (and use different color and/or style)\nQ: Is the polution getting better along the years or not?\nax = polution_ts['2008':].resample('Y').mean().plot(figsize=(4, 4))\n# Sample by year (A pour Annual) or Y for Year\nplt.ylim(0, 50)\nplt.title(\"Pollution evolution: \\n yearly average in Paris\")\nplt.ylabel(\"Concentration (µg/m³)\")\nplt.xlabel(\"Year\")\n# Load colors\nsns.set_palette(\"GnBu_d\", n_colors=7)\npolution_ts['weekday'] = polution_ts.index.weekday  # Monday=0, Sunday=6\npolution_ts['weekend'] = polution_ts['weekday'].isin([5, 6])\n\ndays = ['Monday', 'Tuesday', 'Wednesday',\n        'Thursday', 'Friday', 'Saturday', 'Sunday']\n\npolution_week_no2 = polution_ts.groupby(['weekday', polution_ts.index.hour])[\n    'NO2'].mean().unstack(level=0)\npolution_week_03 = polution_ts.groupby(['weekday', polution_ts.index.hour])[\n    'O3'].mean().unstack(level=0)\nplt.show()\nfig, axes = plt.subplots(2, 1, figsize=(7, 7), sharex=True)\n\npolution_week_no2.plot(ax=axes[0])\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\naxes[0].set_xlabel(\"Heure de la journée\")\naxes[0].set_title(\n    \"Profil journalier de la pollution au NO2: effet du weekend?\")\naxes[0].set_xticks(np.arange(0, 24))\naxes[0].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[0].set_ylim(0, 60)\n\npolution_week_03.plot(ax=axes[1])\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\naxes[1].set_xlabel(\"Heure de la journée\")\naxes[1].set_title(\"Profil journalier de la pollution au O3: effet du weekend?\")\naxes[1].set_xticks(np.arange(0, 24))\naxes[1].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[1].set_ylim(0, 70)\naxes[0].legend().set_visible(False)\n# ax.legend()\naxes[1].legend(labels=days, loc='lower left', bbox_to_anchor=(1, 0.1))\n\nplt.tight_layout()\nimport calendar\npolution_ts['month'] = polution_ts.index.month  # Janvier=0, .... Decembre=11\npolution_ts['month'] = polution_ts['month'].apply(lambda x:\n                                                  calendar.month_abbr[x])\npolution_ts.head()\n\npolution_month_no2 = polution_ts.groupby(['month', polution_ts.index.hour])[\n    'NO2'].mean().unstack(level=0)\npolution_month_03 = polution_ts.groupby(['month', polution_ts.index.hour])[\n    'O3'].mean().unstack(level=0)\nsns.set_palette(\"Paired\", n_colors=12)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 7), sharex=True)\n\npolution_month_no2.plot(ax=axes[0])\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\naxes[0].set_xlabel(\"Heure de la journée\")\naxes[0].set_title(\n    \"Profil journalier de la pollution au NO2: effet du weekend?\")\naxes[0].set_xticks(np.arange(0, 24))\naxes[0].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[0].set_ylim(0, 90)\n\npolution_month_03.plot(ax=axes[1])\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\naxes[1].set_xlabel(\"Heure de la journée\")\naxes[1].set_title(\"Profil journalier de la pollution au O3: effet du weekend?\")\naxes[1].set_xticks(np.arange(0, 24))\naxes[1].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[1].set_ylim(0, 90)\naxes[0].legend().set_visible(False)\n# ax.legend()\naxes[1].legend(labels=calendar.month_name[1:], loc='lower left',\n               bbox_to_anchor=(1, 0.1))\nplt.tight_layout()\n\n\n\n\nhttps://www.data.gouv.fr/fr/datasets/accidents-de-velo-en-france/\nPossible visualization: https://koumoul.com/en/datasets/accidents-velos\n\n\nurl = \"https://koumoul.com/s/data-fair/api/v1/datasets/accidents-velos/raw\"\npath_target = \"./bicycle_db.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n# df: data frame\ndf_bikes = pd.read_csv(\"bicycle_db.csv\", na_values=\"\",\n                       converters={'data': str, 'heure': str})\nget_ipython().system('head -5 ./bicycle_db.csv')\npd.options.display.max_columns = 40\ndf_bikes.head()\n\n\n```python\ndf_bikes['existence securite'].unique()\ndf_bikes['gravite accident'].unique()\n\n\ndf_bikes['date'].hasnans\ndf_bikes['heure'].hasnans\npd.options.display.max_rows = 20\ndf_bikes.iloc[400:402]\nRemove missing hours cases by np.nan:\ndf_bikes['heure'] = df_bikes['heure'].replace('', np.nan)\ndf_bikes.iloc[400:402]\ndf_bikes.dropna(subset=['heure'], inplace=True)\ndf_bikes.iloc[399:402]\n\n\n\nCan you find the starting day and the ending day of the study automatically? Hint: sort the data; you can sort the data by time, say with df.sort(‘Time’)\ndf_bikes['date'] + ' ' + df_bikes['heure'] + ':00'\n# ADAPT OLD to create the df_bikes['Time']\n\ntime_improved = pd.to_datetime(df_bikes['date'] +\n                               ' ' + df_bikes['heure'] + ':00',\n                               format='%Y-%m-%d %H:%M')\n\n# Where d = day, m=month, Y=year, H=hour, M=minutes\n# create correct timing format in the dataframe\ndf_bikes['Time'] = time_improved\ndf_bikes.set_index('Time', inplace=True)\n# remove useles columns\ndel df_bikes['heure']\ndel df_bikes['date']\ndf_bikes.info()\ndf_bike2 = df_bikes[['gravite accident', 'existence securite',\n                             'age', 'sexe']]\ndf_bike2['existence securite'] = df_bike2['existence securite'].replace(np.nan, \"Inconnu\")\ndf_bike2.dropna(inplace=True)\n\n\n\nPeform an analysis so that you can check the benefit or not of wearing helmet to save your life. Beware preprocessing needed to use pd.crosstab, pivot_table to avoid issues.\n\ngroup = df_bike2.pivot_table(columns='existence securite',\n                             index=['gravite accident', 'sexe'],\n                             aggfunc={'age': 'count'}, margins=True)\ngroup\npd.crosstab(df_bike2['existence securite'],\n            df_bike2['gravite accident'], normalize='index') * 100\npd.crosstab(df_bike2['existence securite'],\n            df_bike2['gravite accident'], values=df_bike2['age'],\n            aggfunc='count', normalize='index') * 100\n\n\n\nAre men and women dying equally on a bike ?  Peform an analysis to check differences between men and woman survival ?\nidx_dead = df_bikes['gravite accident'] == '3 - Tué'\ndf_deads = df_bikes[idx_dead]\ndf_gravite = df_deads.groupby('sexe').size() / idx_dead.sum()\ndf_gravite\ndf_bikes.groupby('sexe').size()  / df_bikes.shape[0]\npd.crosstab(df_bike2['sexe'],\n            df_bike2['gravite accident'],\n            values=df_bike2['age'], aggfunc='count',\n            normalize='columns', margins=True) * 100\n\n\n\nNote: information on the level of bike practice by men/women is missing…\n\n\n\nPeform an analysis to check when the accidents are occuring during the week.\ndf_bikes\n```python\n\n\n```python\n# Chargement des couleurs\nsns.set_palette(\"GnBu_d\", n_colors=7)\n\ndf_bikes['weekday'] = df_bikes.index.weekday  # Monday=0, Sunday=6\n\naccidents_week = df_bikes.groupby(['weekday', df_bikes.index.hour])[\n    'sexe'].count().unstack(level=0)\n\nfig, axes = plt.subplots(1, 1, figsize=(7, 7))\naccidents_week.plot(ax=axes)\naxes.set_ylabel(\"Accidents\")\naxes.set_xlabel(\"Heure de la journée\")\naxes.set_title(\n    \"Profil journalier des accidents: effet du weekend?\")\naxes.set_xticks(np.arange(0, 24))\naxes.set_xticklabels(np.arange(0, 24), rotation=45)\n# axes.set_ylim(0, 6)\naxes.legend(labels=days, loc='lower left', bbox_to_anchor=(1, 0.1))\nplt.legend()\nplt.tight_layout()\ndf_bikes.groupby(['weekday', df_bikes.index.hour])[\n    'sexe'].count()\n\n\n\nPeform an analysis to check when the accidents are occuring during the week.\ndf_bikes['month'] = df_bikes.index.month  # Janvier=0, .... Decembre=11\ndf_bikes['month'] = df_bikes['month'].apply(lambda x: calendar.month_abbr[x])\ndf_bikes.head()\n\nsns.set_palette(\"GnBu_d\", n_colors=12)  # sns.set_palette(\"colorblind\",...)\n\ndf_bikes_month = df_bikes.groupby(['month', df_bikes.index.hour])[\n    'age'].count().unstack(level=0)\n\nfig, axes = plt.subplots(1, 1, figsize=(7, 7), sharex=True)\n\ndf_bikes_month.plot(ax=axes)\naxes.set_ylabel(\"Concentration (µg/m³)\")\naxes.set_xlabel(\"Heure de la journée\")\naxes.set_title(\n    \"Profil journalier de la pollution au NO2: effet du weekend?\")\naxes.set_xticks(np.arange(0, 24))\naxes.set_xticklabels(np.arange(0, 24), rotation=45)\n# axes.set_ylim(0, 90)\naxes.legend(labels=calendar.month_name[1:], loc='lower left',\n            bbox_to_anchor=(1, 0.1))\n\nplt.tight_layout()\n\n\n\nPeform an analysis to check when the accidents are occuring by departement.\nimport pygal\n# First install if needed for maps:\n# pip install pygal\n# andpip install pygal_maps_frpip install pygal_maps_fr\n# pip install pygal_maps_fr\n\n# Departement population: https://public.opendatasoft.com/explore/dataset/population-francaise-par-departement-2018/table/?disjunctive.departement&location=7,47.12995,3.41125&basemap=jawg.streets\npath_target = \"./dpt_population.csv\"\nurl = \"https://public.opendatasoft.com/explore/dataset/population-francaise-par-departement-2018/download/?format=csv&timezone=Europe/Berlin&lang=en&use_labels_for_header=true&csv_separator=%3B\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n# Departement area: https://www.regions-et-departements.fr/departements-francais#departements_fichiers\npath_target = \"./dpt_area.csv\"\nurl = \"https://www.regions-et-departements.fr/fichiers/departements-francais.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n\ndf_dtp_pop = pd.read_csv(\"dpt_population.csv\", sep=\";\", low_memory=False)\ndf_dtp_area = pd.read_csv(\"dpt_area.csv\", sep=\"\\t\", low_memory=False, skiprows=[102, 103, 104])\ndf_dtp_area['NUMÉRO']\n\n\ndf_dtp_area.set_index('NUMÉRO', inplace=True)\ndf_dtp_pop.set_index('Code Département', inplace=True)\ndf_dtp_pop.sort_index(inplace=True)\n\nfr_chart = pygal.maps.fr.Departments(human_readable=True)\ndisplay = \"ratio_accident\"\n\n\nif display is \"ratio_accident\":\n    fr_chart.title = 'Accidents by departement'\n    gd = df_bikes.groupby(['departement']).size()\n    gd = (gd / df_dtp_pop['Population'])  # mean accident per habitant\nelse:\n    fr_chart.title = 'Deaths by departement'\n    df_deads = df_bikes[df_bikes['gravite accident']=='3 - Tué']\n    df_gravite = df_deads.groupby('departement').size()\n    # gd = df_bikes.groupby(['departement']).aggregate(lambda: x->sum(x))\n    gd = (df_gravite / df_dtp_pop['Population'])  # mean deaths per habitant\n\n# Area normalization\nnormalization = True\nif normalization is True:\n    gd = (gd / df_dtp_area['SUPERFICIE (km²)'])\ngd.dropna(inplace=True)   # anoying NA due to 1 vs 01 in datasets\nfr_chart.add('Accidents', gd.to_dict())\nfr_chart.render_in_browser()\n# fr_chart.render_to_file('./chatr.svg')  # Write the chart in a specified file"
  },
  {
    "objectID": "Courses/Python-modules/tp.html",
    "href": "Courses/Python-modules/tp.html",
    "title": "Creating a python module",
    "section": "",
    "text": "You already know it: this is a set of python functions and statements, and this is what you import at the beginning of your python functions.\n\n\nIndeed, a module can simply be a single file:\n\nimport fibo\n\nThis does not enter the names of the functions defined in fibo directly in the current symbol table though; it only enters the module name fibo there. Using the module name you can access the functions:\n\nfibo.fib(1000)\n\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 \n\n\n\nfibo.fib2(100)\n\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n\n\n\nfibo.__name__\n\n'fibo'\n\n\n\nfibo.__file__\n\n'/home/jsalmon/Documents/Mes_cours/Montpellier/HAX712X/Courses/Python-modules/fibo.py'\n\n\nWhen importing a module, several methods are (automatically) defined. Their names are usually prefixed and suffixed by the symbol __, e.g.,\n\n\n\nYou can also import a full directory (containing many python files stored in sub-folder). Python looks for a folder located in sys.path list. You have already imported the numpy module, for numerical analysis with python:\n\nimport numpy as np\nprint(np.array([0, 1, 2, 3]).reshape(2, 2))\nprint(np.array([0, 1, 2, 3]).mean())\n\n[[0 1]\n [2 3]]\n1.5\n\n\nIn fact, you have imported the following folder:\nnp.__path__\nDepending on your installation you might obtain ['/usr/lib/python3.9/site-packages/numpy'] or ['/home/username/anaconda3/lib/python3.7/site-packages/numpy'] if you installed with anaconda.\nMore precisely this file\n>>> np.__file__\n'/usr/lib/python3.9/site-packages/numpy/__init__.py'\nor\n>>> np.__file__\n['/home/username/anaconda3/lib/python3.7/site-packages/numpy/__init__.py']\nAny (sub-)directory of your python module should contain an __init__.py file!\nUseful tips:\n\nThe __init__.py file can contain a list of function to be loaded when the module is imported. It allows to expose functions to user in a concise way.\nYou can also import modules with relative path, using ., .., ..., etc. See: https://realpython.com/absolute-vs-relative-python-imports/\n\n\n\n\nThe built-in function dir() is used to find out which names a module defines. It returns a sorted list of strings:\n\nimport fibo, numpy\ndir(fibo)\n\n['__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'fib',\n 'fib2']\n\n\n\nfibo.__dir__\n\n<function __dir__>\n\n\n\ndir(numpy)\n\n['ALLOW_THREADS',\n 'AxisError',\n 'BUFSIZE',\n 'CLIP',\n 'ComplexWarning',\n 'DataSource',\n 'ERR_CALL',\n 'ERR_DEFAULT',\n 'ERR_IGNORE',\n 'ERR_LOG',\n 'ERR_PRINT',\n 'ERR_RAISE',\n 'ERR_WARN',\n 'FLOATING_POINT_SUPPORT',\n 'FPE_DIVIDEBYZERO',\n 'FPE_INVALID',\n 'FPE_OVERFLOW',\n 'FPE_UNDERFLOW',\n 'False_',\n 'Inf',\n 'Infinity',\n 'MAXDIMS',\n 'MAY_SHARE_BOUNDS',\n 'MAY_SHARE_EXACT',\n 'ModuleDeprecationWarning',\n 'NAN',\n 'NINF',\n 'NZERO',\n 'NaN',\n 'PINF',\n 'PZERO',\n 'RAISE',\n 'RankWarning',\n 'SHIFT_DIVIDEBYZERO',\n 'SHIFT_INVALID',\n 'SHIFT_OVERFLOW',\n 'SHIFT_UNDERFLOW',\n 'ScalarType',\n 'Tester',\n 'TooHardError',\n 'True_',\n 'UFUNC_BUFSIZE_DEFAULT',\n 'UFUNC_PYVALS_NAME',\n 'VisibleDeprecationWarning',\n 'WRAP',\n '_CopyMode',\n '_NoValue',\n '_UFUNC_API',\n '__NUMPY_SETUP__',\n '__all__',\n '__builtins__',\n '__cached__',\n '__config__',\n '__deprecated_attrs__',\n '__dir__',\n '__doc__',\n '__expired_functions__',\n '__file__',\n '__getattr__',\n '__git_version__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '__version__',\n '_add_newdoc_ufunc',\n '_distributor_init',\n '_financial_names',\n '_from_dlpack',\n '_globals',\n '_mat',\n '_pytesttester',\n '_version',\n 'abs',\n 'absolute',\n 'add',\n 'add_docstring',\n 'add_newdoc',\n 'add_newdoc_ufunc',\n 'alen',\n 'all',\n 'allclose',\n 'alltrue',\n 'amax',\n 'amin',\n 'angle',\n 'any',\n 'append',\n 'apply_along_axis',\n 'apply_over_axes',\n 'arange',\n 'arccos',\n 'arccosh',\n 'arcsin',\n 'arcsinh',\n 'arctan',\n 'arctan2',\n 'arctanh',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'argwhere',\n 'around',\n 'array',\n 'array2string',\n 'array_equal',\n 'array_equiv',\n 'array_repr',\n 'array_split',\n 'array_str',\n 'asanyarray',\n 'asarray',\n 'asarray_chkfinite',\n 'ascontiguousarray',\n 'asfarray',\n 'asfortranarray',\n 'asmatrix',\n 'asscalar',\n 'atleast_1d',\n 'atleast_2d',\n 'atleast_3d',\n 'average',\n 'bartlett',\n 'base_repr',\n 'binary_repr',\n 'bincount',\n 'bitwise_and',\n 'bitwise_not',\n 'bitwise_or',\n 'bitwise_xor',\n 'blackman',\n 'block',\n 'bmat',\n 'bool8',\n 'bool_',\n 'broadcast',\n 'broadcast_arrays',\n 'broadcast_shapes',\n 'broadcast_to',\n 'busday_count',\n 'busday_offset',\n 'busdaycalendar',\n 'byte',\n 'byte_bounds',\n 'bytes0',\n 'bytes_',\n 'c_',\n 'can_cast',\n 'cast',\n 'cbrt',\n 'cdouble',\n 'ceil',\n 'cfloat',\n 'char',\n 'character',\n 'chararray',\n 'choose',\n 'clip',\n 'clongdouble',\n 'clongfloat',\n 'column_stack',\n 'common_type',\n 'compare_chararrays',\n 'compat',\n 'complex128',\n 'complex256',\n 'complex64',\n 'complex_',\n 'complexfloating',\n 'compress',\n 'concatenate',\n 'conj',\n 'conjugate',\n 'convolve',\n 'copy',\n 'copysign',\n 'copyto',\n 'core',\n 'corrcoef',\n 'correlate',\n 'cos',\n 'cosh',\n 'count_nonzero',\n 'cov',\n 'cross',\n 'csingle',\n 'ctypeslib',\n 'cumprod',\n 'cumproduct',\n 'cumsum',\n 'datetime64',\n 'datetime_as_string',\n 'datetime_data',\n 'deg2rad',\n 'degrees',\n 'delete',\n 'deprecate',\n 'deprecate_with_doc',\n 'diag',\n 'diag_indices',\n 'diag_indices_from',\n 'diagflat',\n 'diagonal',\n 'diff',\n 'digitize',\n 'disp',\n 'divide',\n 'divmod',\n 'dot',\n 'double',\n 'dsplit',\n 'dstack',\n 'dtype',\n 'e',\n 'ediff1d',\n 'einsum',\n 'einsum_path',\n 'emath',\n 'empty',\n 'empty_like',\n 'equal',\n 'errstate',\n 'euler_gamma',\n 'exp',\n 'exp2',\n 'expand_dims',\n 'expm1',\n 'extract',\n 'eye',\n 'fabs',\n 'fastCopyAndTranspose',\n 'fft',\n 'fill_diagonal',\n 'find_common_type',\n 'finfo',\n 'fix',\n 'flatiter',\n 'flatnonzero',\n 'flexible',\n 'flip',\n 'fliplr',\n 'flipud',\n 'float128',\n 'float16',\n 'float32',\n 'float64',\n 'float_',\n 'float_power',\n 'floating',\n 'floor',\n 'floor_divide',\n 'fmax',\n 'fmin',\n 'fmod',\n 'format_float_positional',\n 'format_float_scientific',\n 'format_parser',\n 'frexp',\n 'frombuffer',\n 'fromfile',\n 'fromfunction',\n 'fromiter',\n 'frompyfunc',\n 'fromregex',\n 'fromstring',\n 'full',\n 'full_like',\n 'gcd',\n 'generic',\n 'genfromtxt',\n 'geomspace',\n 'get_array_wrap',\n 'get_include',\n 'get_printoptions',\n 'getbufsize',\n 'geterr',\n 'geterrcall',\n 'geterrobj',\n 'gradient',\n 'greater',\n 'greater_equal',\n 'half',\n 'hamming',\n 'hanning',\n 'heaviside',\n 'histogram',\n 'histogram2d',\n 'histogram_bin_edges',\n 'histogramdd',\n 'hsplit',\n 'hstack',\n 'hypot',\n 'i0',\n 'identity',\n 'iinfo',\n 'imag',\n 'in1d',\n 'index_exp',\n 'indices',\n 'inexact',\n 'inf',\n 'info',\n 'infty',\n 'inner',\n 'insert',\n 'int0',\n 'int16',\n 'int32',\n 'int64',\n 'int8',\n 'int_',\n 'intc',\n 'integer',\n 'interp',\n 'intersect1d',\n 'intp',\n 'invert',\n 'is_busday',\n 'isclose',\n 'iscomplex',\n 'iscomplexobj',\n 'isfinite',\n 'isfortran',\n 'isin',\n 'isinf',\n 'isnan',\n 'isnat',\n 'isneginf',\n 'isposinf',\n 'isreal',\n 'isrealobj',\n 'isscalar',\n 'issctype',\n 'issubclass_',\n 'issubdtype',\n 'issubsctype',\n 'iterable',\n 'ix_',\n 'kaiser',\n 'kernel_version',\n 'kron',\n 'lcm',\n 'ldexp',\n 'left_shift',\n 'less',\n 'less_equal',\n 'lexsort',\n 'lib',\n 'linalg',\n 'linspace',\n 'little_endian',\n 'load',\n 'loadtxt',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'logaddexp',\n 'logaddexp2',\n 'logical_and',\n 'logical_not',\n 'logical_or',\n 'logical_xor',\n 'logspace',\n 'longcomplex',\n 'longdouble',\n 'longfloat',\n 'longlong',\n 'lookfor',\n 'ma',\n 'mask_indices',\n 'mat',\n 'math',\n 'matmul',\n 'matrix',\n 'matrixlib',\n 'max',\n 'maximum',\n 'maximum_sctype',\n 'may_share_memory',\n 'mean',\n 'median',\n 'memmap',\n 'meshgrid',\n 'mgrid',\n 'min',\n 'min_scalar_type',\n 'minimum',\n 'mintypecode',\n 'mod',\n 'modf',\n 'moveaxis',\n 'msort',\n 'multiply',\n 'nan',\n 'nan_to_num',\n 'nanargmax',\n 'nanargmin',\n 'nancumprod',\n 'nancumsum',\n 'nanmax',\n 'nanmean',\n 'nanmedian',\n 'nanmin',\n 'nanpercentile',\n 'nanprod',\n 'nanquantile',\n 'nanstd',\n 'nansum',\n 'nanvar',\n 'nbytes',\n 'ndarray',\n 'ndenumerate',\n 'ndim',\n 'ndindex',\n 'nditer',\n 'negative',\n 'nested_iters',\n 'newaxis',\n 'nextafter',\n 'nonzero',\n 'not_equal',\n 'numarray',\n 'number',\n 'obj2sctype',\n 'object0',\n 'object_',\n 'ogrid',\n 'oldnumeric',\n 'ones',\n 'ones_like',\n 'os',\n 'outer',\n 'packbits',\n 'pad',\n 'partition',\n 'percentile',\n 'pi',\n 'piecewise',\n 'place',\n 'poly',\n 'poly1d',\n 'polyadd',\n 'polyder',\n 'polydiv',\n 'polyfit',\n 'polyint',\n 'polymul',\n 'polynomial',\n 'polysub',\n 'polyval',\n 'positive',\n 'power',\n 'printoptions',\n 'prod',\n 'product',\n 'promote_types',\n 'ptp',\n 'put',\n 'put_along_axis',\n 'putmask',\n 'quantile',\n 'r_',\n 'rad2deg',\n 'radians',\n 'random',\n 'ravel',\n 'ravel_multi_index',\n 'real',\n 'real_if_close',\n 'rec',\n 'recarray',\n 'recfromcsv',\n 'recfromtxt',\n 'reciprocal',\n 'record',\n 'remainder',\n 'repeat',\n 'require',\n 'reshape',\n 'resize',\n 'result_type',\n 'right_shift',\n 'rint',\n 'roll',\n 'rollaxis',\n 'roots',\n 'rot90',\n 'round',\n 'round_',\n 'row_stack',\n 's_',\n 'safe_eval',\n 'save',\n 'savetxt',\n 'savez',\n 'savez_compressed',\n 'sctype2char',\n 'sctypeDict',\n 'sctypes',\n 'searchsorted',\n 'select',\n 'set_numeric_ops',\n 'set_printoptions',\n 'set_string_function',\n 'setbufsize',\n 'setdiff1d',\n 'seterr',\n 'seterrcall',\n 'seterrobj',\n 'setxor1d',\n 'shape',\n 'shares_memory',\n 'short',\n 'show_config',\n 'sign',\n 'signbit',\n 'signedinteger',\n 'sin',\n 'sinc',\n 'single',\n 'singlecomplex',\n 'sinh',\n 'size',\n 'sometrue',\n 'sort',\n 'sort_complex',\n 'source',\n 'spacing',\n 'split',\n 'sqrt',\n 'square',\n 'squeeze',\n 'stack',\n 'std',\n 'str0',\n 'str_',\n 'string_',\n 'subtract',\n 'sum',\n 'swapaxes',\n 'sys',\n 'take',\n 'take_along_axis',\n 'tan',\n 'tanh',\n 'tensordot',\n 'test',\n 'testing',\n 'tile',\n 'timedelta64',\n 'trace',\n 'tracemalloc_domain',\n 'transpose',\n 'trapz',\n 'tri',\n 'tril',\n 'tril_indices',\n 'tril_indices_from',\n 'trim_zeros',\n 'triu',\n 'triu_indices',\n 'triu_indices_from',\n 'true_divide',\n 'trunc',\n 'typecodes',\n 'typename',\n 'ubyte',\n 'ufunc',\n 'uint',\n 'uint0',\n 'uint16',\n 'uint32',\n 'uint64',\n 'uint8',\n 'uintc',\n 'uintp',\n 'ulonglong',\n 'unicode_',\n 'union1d',\n 'unique',\n 'unpackbits',\n 'unravel_index',\n 'unsignedinteger',\n 'unwrap',\n 'use_hugepage',\n 'ushort',\n 'vander',\n 'var',\n 'vdot',\n 'vectorize',\n 'version',\n 'void',\n 'void0',\n 'vsplit',\n 'vstack',\n 'warnings',\n 'where',\n 'who',\n 'zeros',\n 'zeros_like']\n\n\n\nnumpy.__dir__\n\n<function numpy.__dir__()>\n\n\n\nTo list every element in your symbol table simply call dir().\nReference: https://docs.python.org/3/tutorial/modules.html#the-dir-function\n\n\n\nA namespace is a set of names (functions, variables, etc.). Different namespaces can co-exist at a given time but are completely isolated. In this way, you can control which function you are using.\nA namespace containing all the built-in names is created when we start the Python interpreter and exists as long we don’t exit.\n\ncos(3)\n\nNameError: name 'cos' is not defined\n\n\nYou need to import a package for mathematical functions:\n\nimport math, numpy as np\nprint(math.cos(3), np.cos(3))\n\n-0.9899924966004454 -0.9899924966004454\n\n\nReferences: https://www.programiz.com/python-programming/namespace\n\n\n\nWhen a module named spam is imported, the interpreter first searches for a built-in module with that name. If not found, it then searches for a file named spam.py in a list of directories given by the variable sys.path. The variable sys.path is initialized from these locations:\n\nThe directory containing the input script (or the current directory when no file is specified).\nThe environment variable PYTHONPATH (a list of directory names, with the same syntax as the shell variable PATH).\n\nReference: https://docs.python.org/3/tutorial/modules.html#the-module-search-path\n\n\n\nFind the loader for a module, optionally within the specified path.\n\nimport importlib\nspam_spec = importlib.util.find_spec(\"spam\")\nfound = spam_spec is not None\nfound\n\nFalse\n\n\nNow\n\nimport numpy\nnumpy_spec = importlib.util.find_spec(\"numpy\")\nprint(numpy_spec)\n\nModuleSpec(name='numpy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f375c2eb790>, origin='/home/jsalmon/anaconda3/lib/python3.8/site-packages/numpy/__init__.py', submodule_search_locations=['/home/jsalmon/anaconda3/lib/python3.8/site-packages/numpy'])\n\n\nshould return more information and where the loader is.\nSee https://docs.python.org/3/library/importlib.html#importlib.find_loader an https://stackoverflow.com/questions/14050281/how-to-check-if-a-python-module-exists-without-importing-it\n\n\n\nA module can contain executable statements as well as function definitions. These statements are intended to initialize the module. They are executed only the first time the module name is encountered in an import statement.\nTo force a module to be reloaded, you can use importlib.reload().\nSee: https://docs.python.org/3/library/importlib.html#importlib.reload\nRemark: when using ipython (interactive python, an ancestor of the jupyter notebook), one can use the “magic” command %autoreload 2, cf. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload\n\n\n\nTo speed up loading modules, python caches the compiled version of each module in the __pycache__ directory under the name module.version.pyc, where the version encodes the format of the compiled file; it generally contains the python version number.\nFor example, in CPython release 3.3 the compiled version of spam.py would be cached as __pycache__/spam.cpython-33.pyc. This naming convention allows compiled modules from different releases and different versions of python to coexist.\nUseful git tip:\n\nyou should add __pycache__ entry in your .gitignore file to avoid to add compiled python file to your project."
  },
  {
    "objectID": "Courses/Python-modules/tp.html#the-python-package-index-pypi-repository",
    "href": "Courses/Python-modules/tp.html#the-python-package-index-pypi-repository",
    "title": "Creating a python module",
    "section": "The Python Package Index (Pypi) repository",
    "text": "The Python Package Index (Pypi) repository\nThe python Package Index, abbreviated as PyPI, is the official third-party software repository for python. PyPI primarily hosts python packages in the form of archives called sdists (source distributions) or pre-compiled “wheels”.\n\nExercise\n\nGo to https://test.pypi.org/ and describe the aim of this repository.\n\n\n\nPip\npip is a de facto standard package-management system used to install and manage software packages from PyPi.\n$ pip install some-package-name\n$ pip uninstall some-package-name\n$ pip search some-package-name\n\n\nExercise\n\nInstall the modules pooch, setuptools, pandas, pygal and pygal_maps_fr. Beware, you should use the option --user to force the installation in your home directory.\nList all the package in your venv using pip.\n\nIt is possible to install a local module with pip\n$ pip install /path/to/my/local/module\nwhere /path/to/my/local/module is the path to the module. But if some changes occur in the /path/to/my/local/module folder, the module will not be reloaded. This might be annoying during the development stage. To force python to reload the module at each change call, consider the -e option:\n$ pip install -e /path/to/my/local/module"
  },
  {
    "objectID": "Courses/Python-modules/tp.html#creating-a-python-module",
    "href": "Courses/Python-modules/tp.html#creating-a-python-module",
    "title": "Creating a python module",
    "section": "Creating a python module",
    "text": "Creating a python module\nReferences: https://python-packaging.readthedocs.io/en/latest/\n\nPicking A Name\nPython module/package names should generally follow the following constraints:\n\nAll lowercase\nUnique on PyPI, even if you do not want to make your package publicly available (you might want to specify it privately as a dependency later)\nUnderscore-separated or no word separators at all, and do not use hyphens (i.e., use _ not -).\n\nWe are going to create a module called biketrauma to visualize the bicycle_db used in the the following lectures that can be found at https://koumoul.com/s/data-fair/api/v1/datasets/accidents-velos/raw.\n\n\nModule structure\nThe initial directory structure for biketrauma should look like this:\npackaging_tutorial/\n    biketrauma/\n        __init__.py\n        data/\n    setup.py\n    .gitignore\nThe top level directory is the root of our Version Control System (e.g. git) repository packaging_tutorial.git. The sub-directory, biketrauma, is the actual Python module.\n\n\nExercise:\nWe are going to create a new python module that can be used to visualize the bike dataset.\n\nCreate a new folder ~/packaging_tutorial/ and initialize a git in it.\nCreate a .gitignore file to ignore __pycache__, .vscode directories and files containing the string egg-info or dist in their name as well.\nPush your work into a new repository on your github.\nCreate a sub-folder ~/packaging_tutorial/biketrauma. This is where our python module will be stored.\nCreate a ~/packaging_tutorial/biketrauma/__init__.py file where a string __version__ defined at 0.0.1.\nCreate an empty sub-folder ~/packaging_tutorial/biketrauma/data locally on your computer/session. How to add it to git? (Hint: .gitkeep)\nCreate an empty ~/packaging_tutorial/setup.py file.\nCommit and push into your repository.\n\nRead also: https://packaging.python.org/guides/single-sourcing-package-version/.\n\n\nSub-modules\nThe final directory structure of our module will look like:\n  packaging_tutorial/\n      biketrauma/\n          __init__.py\n          io/\n            __init__.py\n          preprocess/\n            __init__.py\n          vis/\n            __init__.py\n          data/\n      setup.py\n      script.py\n\n\nExercise\nAdd some python files in the modules_files folder:\n\nAdd some sub-folders to biketrauma called io (for input/output), preprocess, vis (for visualization). Copy the script.py into the root folder.\nPopulate the preprocess sub-module with the get_accident.py file (see the git repo of the course in the subfolder Courses/Python-modules/modules_files)\nPopulate the vis sub-module with the plot_location.py file\nPopulate the io sub-module with the file Load_db.py (it downloads the bike data-set). At the loading step your sub-module should create the variables\n\nurl_db = \"https://koumoul.com/s/data-fair/api/v1/datasets/accidents-velos/raw\"\npath_target = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"..\", \"data\", \"bicycle_db.csv\")\n\n\nAdding additional files\nIn order to load the functions in the io, preprocess and vis sub-modules, you can add the following lines to the ~/packaging_tutorial/biketrauma/__init__.py:\nfrom .io.Load_db import Load_db\nfrom .vis.plot_location import plot_location\nfrom .preprocess.get_accident import get_accident\n\n\nExercise\n\nCheck that your module does work by launching the script.py script\nCreate a file format_date.py in the biketrauma.preprocess module in which a function format_date format the date of the data-set in international format.\nThis function should accessible with the command:\n\n>>>import biketrauma\n>>>df = biketrauma.Load_db().save_as_df()\n>>>df_nicely_formated = biketrauma.format_date(df)\n\n\nPackage the module with setuptools\nThe main setup configuration file, setup.py, should contain a single call to setuptools.setup(), like so:\nfrom setuptools import setup\nfrom biketrauma import __version__ as current_version\n\nsetup(\n  name='biketrauma',\n  version=current_version,\n  description='Visualization of a bicycle accident db',\n  url='http://github.com/xxxxxxxxxxx.git',\n  author='xxxxxxxxxxx',\n  author_email='xxxxxxxxxx@xxxxxxxxxxxxx.xxx',\n  license='MIT',\n  packages=['biketrauma','biketrauma.io', 'biketrauma.preprocess', 'biketrauma.vis'],\n  zip_safe=False\n)\nTo create a sdist package (a source distribution):\n$ cd ~/packaging_tutorial/\n$ python setup.py sdist\nThis will create dist/biketrauma-0.0.1.tar.gz inside the top-level directory. You can now install it with\n$ pip install ~/packaging_tutorial/dist/biketrauma-0.0.1.tar.gz\nSee https://setuptools.readthedocs.io/en/latest/setuptools.html and https://packaging.python.org/tutorials/packaging-projects/\n\n\nAdd requirement file\nTo get a list of the installed package in your current venv, you can use the following command:\n$ pip freeze > requirements.txt\nUnfortunately, it may generate a way too large collection of packages dependencies. To get a sparser list, you can use pipreqs\n\n\nExercise:\n\nCreate a minimal requirements.txt file with pipreqs. Add it to the biketrauma module.\n\n\n\nUpload on PyPI\ntwine is a utility for publishing Python packages on PyPI. We are going to use the test repository https://test.pypi.org/.\n\n\nExercise\n\nCreate an account on the PyPI test repository\n\nThis is quite easy to upload a python module on PyPI:\n\nCreate some distributions in the normal way:\n\n$ python setup.py sdist bdist_wheel\n\nUpload with twine to Test PyPI and verify things look right. Twine will automatically prompt for your username and password:\n\n$ twine upload --repository-url https://test.pypi.org/legacy/ dist/*\nusername: ...\npassword: ...\n\nUpload to PyPI:\n\n$ twine upload dist/*\nMore documentation on using twine to upload packages to PyPI is in the Python Packaging User Guide.\nReferences: https://pypi.org/project/twine/"
  },
  {
    "objectID": "Courses/ScipyNumpy/tp.html",
    "href": "Courses/ScipyNumpy/tp.html",
    "title": "Animation display with python",
    "section": "",
    "text": "# SciPy - scientific library in python\nAdapted from\n## Introduction\nSciPy build upon NumPy.\nAmong others, SciPy deals with:"
  },
  {
    "objectID": "Courses/ScipyNumpy/tp.html#animation-with-matplotlib",
    "href": "Courses/ScipyNumpy/tp.html#animation-with-matplotlib",
    "title": "Animation display with python",
    "section": "Animation with matplotlib",
    "text": "Animation with matplotlib\nVSCode backend cannot handle the example out of the box… Use that one in a terminal.\nfig, ax = plt.subplots()\nxdata, ydata = [], []\n(ln,) = plt.plot([], [], \"ro\")\n\n\ndef init():\n    ax.set_xlim(0, 2 * np.pi)\n    ax.set_ylim(-1, 1)\n    return (ln,)\n\n\ndef update(frame):\n    xdata.append(frame)\n    ydata.append(np.sin(frame))\n    ln.set_data(xdata, ydata)\n    return (ln,)\n\n\nani = FuncAnimation(\n    fig, update, frames=np.linspace(0, 2 * np.pi, 128), init_func=init, blit=True\n)\nplt.show()\nYet, a workaround exists: from IPython.display import HTML\nHTML(ani.to_jshtml())"
  },
  {
    "objectID": "Courses/ScipyNumpy/tp.html#optimization",
    "href": "Courses/ScipyNumpy/tp.html#optimization",
    "title": "Animation display with python",
    "section": "Optimization",
    "text": "Optimization\nGoal: find functions minima or maxima Doc : http://scipy-lectures.github.com/advanced/mathematical_optimization/index.html\nfrom scipy import optimize\n\nFinding a (local!) minima\ndef f(x):\n    return 4 * x ** 3 + (x - 2) ** 2 + x ** 4\n\n\ndef mf(x):\n    return -(4 * x ** 3 + (x - 2) ** 2 + x ** 4)\n\n\nxs = np.linspace(-5, 3, 100)\nplt.figure()\nplt.plot(xs, f(xs))\nplt.show()\nDefault solver for minimization/maximization: fmin_bfgs (see https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\nx_min = optimize.fmin_bfgs(f, x0=-4)\nx_max = optimize.fmin_bfgs(mf, x0=-2)\nx_min2 = optimize.fmin_bfgs(f, x0=2)\n\n\nplt.figure()\nplt.plot(xs, f(xs))\nplt.plot(x_min, f(x_min), \"o\", markersize=10, color=\"orange\")\nplt.plot(x_min2, f(x_min2), \"o\", markersize=10, color=\"red\")\nplt.plot(x_max, f(x_max), \"|\", markersize=20)\nplt.show()\n\n\n EXERCISE : Bassin of attraction\nDraw the points on the curves with 2 different colors : - orange for the points leading to find the left local minima - red for the points leading to the right local minima.\n\n\nFind the zeros of a function\nFind x such that f(x) = 0, with fsolve.\nomega_c = 3.0\n\ndef f(omega):\n    return np.tan(2 * np.pi * omega) - omega_c / omega\n\n\n\nx = np.linspace(1e-8, 3.2, 1000)\ny = f(x)\n\n# remove vertical lines when the function flips sign\nmask = np.where(np.abs(y) > 50)\nx[mask] = y[mask] = np.nan\nplt.plot(x, y)\nplt.plot([0, 3.3], [0, 0], \"k\")\nplt.ylim(-5, 5)\n\noptimize.fsolve(f, 0.72)\noptimize.fsolve(f, 1.1)\noptimize.fsolve(f, np.linspace(0.001, 3, 20))\nnp.unique(np.round(optimize.fsolve(f, np.linspace(0.2, 3, 20)), 3))\n\nmy_zeros = (\n    np.unique((optimize.fsolve(f, np.linspace(0.2, 3, 20)) * 1000).astype(int)) / 1000.0\n)\nplt.figure()\nplt.plot(x, y, label=\"$f$\")\nplt.plot([0, 3.3], [0, 0], \"k\")\nplt.plot(my_zeros, np.zeros(my_zeros.shape), \"o\", label=\"$x : f(x)=0$\")\nplt.legend()\nplt.show()\n\n\nParameters estimation\nfrom scipy.optimize import curve_fit\n\n\ndef f(x, a, b, c):\n    \"\"\"f(x) = a exp(-bx) + c.\"\"\"\n    return a * np.exp(-b * x) + c\n\n\nx = np.linspace(0, 4, 50)\ny = f(x, 2.5, 1.3, 0.5)  # true signal\nyn = y + 0.2 * np.random.randn(len(x))  # noisy added\n\n\nplt.figure()\nplt.plot(x, yn, \".\")\nplt.plot(x, y, \"k\", label=\"$f$\")\nplt.legend()\n\n\n(a, b, c), _ = curve_fit(f, x, yn)\nprint(a, b, c)\nDisplaying\nplt.figure()\nplt.plot(x, yn, \".\", label=\"data\")\nplt.plot(x, y, \"k\", label=\"True $f$\")\nplt.plot(x, f(x, a, b, c), \"--k\", label=\"Estimated $\\hat{f}$\")\nplt.legend()\nplt.show()\nRem: for polynomial fitting, one can directly use numpy\nx = np.linspace(0, 1, 10)\ny = np.sin(x * np.pi / 2.0)\nline = np.polyfit(x, y, deg=10)\nplt.figure()\nplt.plot(x, y, \".\", label=\"data\")\nplt.plot(x, np.polyval(line, x), \"k--\", label=\"polynomial approximation\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Courses/ScipyNumpy/tp.html#interpolation",
    "href": "Courses/ScipyNumpy/tp.html#interpolation",
    "title": "Animation display with python",
    "section": "Interpolation",
    "text": "Interpolation\nfrom scipy.interpolate import interp1d, CubicSpline\n\n\ndef f(x):\n    return np.sin(x)\n\n\nn = np.arange(0, 10)\nx = np.linspace(0, 9, 100)\n\ny_meas = f(n) + 0.1 * np.random.randn(len(n))  # add noise\ny_real = f(x)\n\nlinear_interpolation = interp1d(n, y_meas)\ny_interp1 = linear_interpolation(x)\n\ncubic_interpolation = CubicSpline(n, y_meas)\ny_interp2 = cubic_interpolation(x)\n\n\nplt.figure()\nplt.plot(n, y_meas, \"bs\", label=\"noisy data\")\nplt.plot(x, y_real, \"k\", lw=2, label=\"true function\")\nplt.plot(x, y_interp1, \"r\", label=\"linear interp\")\nplt.plot(x, y_interp2, \"g\", label=\"CubicSpline interp\")\nplt.legend(loc=3)\nplt.show()"
  },
  {
    "objectID": "Courses/ScipyNumpy/tp.html#images",
    "href": "Courses/ScipyNumpy/tp.html#images",
    "title": "Animation display with python",
    "section": "Images",
    "text": "Images\nfrom scipy import ndimage, misc\n\nimg = misc.face()\ntype(img), img.dtype, img.ndim, img.shape\n\n\nprint(2 ** 8)  # uint8-> code sur 256 niveau.\n\nn_1, n_2, n_3 = img.shape\nnp.unique(img)\n\n\n# True image\nplt.figure()\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n\nRGB decomposition\nfig, ax = plt.subplots(3, 2)\nfig.set_size_inches(9, 6.5)\nn_1, n_2, n_3 = img.shape\n\nax[0, 0].imshow(img[:, :, 0], cmap=plt.cm.Reds)\nax[0, 1].hist(img[:, :, 0].reshape(n_1 * n_2), np.arange(0, 256))\n\nax[1, 0].imshow(img[:, :, 1], cmap=plt.cm.Greens)\nax[1, 1].hist(img[:, :, 1].reshape(n_1 * n_2), np.arange(0, 256))\n\nax[2, 0].imshow(img[:, :, 2], cmap=plt.cm.Blues)\nax[2, 1].hist(img[:, :, 2].reshape(n_1 * n_2), np.arange(0, 256))\n\nplt.tight_layout()\nprint(img.flags)  # cannot edit...\nimg_compressed = img.copy()\nimg_compressed.setflags(write=1)\nprint(img_compressed.flags)  # can edit now\n\n\nimg_compressed[img_compressed < 70] = 50\nimg_compressed[(img_compressed >= 70) & (img_compressed < 110)] = 100\nimg_compressed[(img_compressed >= 110) & (img_compressed < 180)] = 150\nimg_compressed[(img_compressed >= 180)] = 200\nplt.figure()\nplt.imshow(img_compressed, cmap=plt.cm.gray)\nplt.axis(\"off\")\nplt.show()\n\n\nConvert a color image in grayscale\nplt.figure()\nplt.imshow(np.mean(img, axis=2), cmap=plt.cm.gray)\nplt.show()\n\n\nBlurring (fr: floutage)\nimg_flou = ndimage.gaussian_filter(img, sigma=20)\nfix, ax = plt.subplots()\nax.imshow(img_flou, cmap=plt.cm.gray)\nax.axis(\"off\")\nplt.show()\nWidget on blurring bandwidth:\n%matplotlib widget\nimport ipywidgets as widgets\n\n# set up plot\nimg_flou = ndimage.gaussian_filter(img, sigma=20)\nfix, ax = plt.subplots()\nax.axis(\"off\")\nplt.show()\n\n@widgets.interact(sigma=(0.1, 200, 0.1))\ndef update(sigma=2):\n    \"\"\"Remove old lines from plot and plot new one\"\"\"\n    # [l.remove() for l in ax.lines]\n    img_flou = ndimage.gaussian_filter(img, sigma)\n    ax.imshow(img_flou, cmap=plt.cm.gray)\n\n\nChanging colors in an image\nimport pooch\nimport os\n\nurl = \"https://upload.wikimedia.org/wikipedia/en/thumb/0/05/Flag_of_Brazil.svg/486px-Flag_of_Brazil.svg.png\"\nname_img =pooch.retrieve(url, known_hash=None)\n\nimg = (255 * plt.imread(name_img)).astype(int)\nimg = img.copy()\nplt.figure()\nplt.imshow(img[:, :, 2], cmap=plt.cm.gray)\n\n\nfig, ax = plt.subplots(3, 2)\nfig.set_size_inches(9, 6.5)\nn_1, n_2, n_3 = img.shape\n\nax[0, 0].imshow(img[:, :, 0], cmap=plt.cm.Reds)\nax[0, 1].hist(img[:, :, 0].reshape(n_1 * n_2), np.arange(0, 256), density=True)\n\nax[1, 0].imshow(img[:, :, 1], cmap=plt.cm.Greens)\nax[1, 1].hist(img[:, :, 1].reshape(n_1 * n_2), np.arange(0, 256), density=True)\n\nax[2, 0].imshow(img[:, :, 2], cmap=plt.cm.Blues)\nax[2, 1].hist(img[:, :, 2].reshape(n_1 * n_2), np.arange(0, 256), density=True)\n\nplt.tight_layout()\n\n\n EXERCISE : Make the Brazilian italianer\n(green white red)\n\n\nXXX TODO\nfind_white_green = img[:, :, 1] > 200\n\n# red part\nimg[:, :, 0][find_dark_green] = 255\n\n#  white part\nimg[:, :, 1][find_white_green] = 255\n\n# blue part\nimg[:, :, 2][find_light_green] = 255\n\nplt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "Courses/ScipyNumpy/tp.html#addionnal-lectures",
    "href": "Courses/ScipyNumpy/tp.html#addionnal-lectures",
    "title": "Animation display with python",
    "section": "Addionnal lectures",
    "text": "Addionnal lectures\n\nhttp://www.scipy.org - The official web page for the SciPy project.\nhttp://docs.scipy.org/doc/scipy/reference/tutorial/index.html - A tutorial on how to get started using SciPy.\nhttps://github.com/scipy/scipy/ - The SciPy source code.\nhttp://scipy-lectures.github.io"
  },
  {
    "objectID": "Courses/Test/tp.html",
    "href": "Courses/Test/tp.html",
    "title": "Unit Tests",
    "section": "",
    "text": "This lecture is extracted and adapted from https://amueller.github.io/COMS4995-s19/slides/aml-02-python-git-testing/#45. See also https://code.visualstudio.com/docs/python/testing for more on pytest with VSCode.\n\n\nTests are small pieces of code ensuring that a part of a program is working as expected.\n\n\nThis is why we place utter most importance in implementing tests along the development steps. It will help you to ensure:\n\nthat code works correctly.\nthat changes do not break anything.\nthat bugs are not reintroduced.\nrobustness to user errors.\ncode is reachable (i.e., it will actually be executed)\n\n\n\n\nThere are different kinds of tests:\n\nUnit tests: they test whether a function does the right thing.\nIntegration tests: they test whether the system/process does the right thing.\nNon-regression tests: they test whether a bug got removed (and will not be reintroduced).\n\n\n\n\nMany coding languages come with their own test framework. In python, we will focus on pytest. It is simple though powerful. pytest searches for all test*.py files and runs all test* methods found. It outputs a nice errors report.\n\n\n\n\n\n\n### Exercise\n\n\n1. Install pytest with pip using the user scheme (--user option) 2. Test if the command pytest is in your PATH (depending on your configuration you will have to add ~/.local/bin in PATH)\n\n\nGet the path to pytest binary\n\n\n\n\n\n\nLet us assume we have a file inc.py containing\ndef inc1(x):\n    return x + 1\n\ndef inc2(x):\n    return x + 2\nThence, the content of test_inc.py is\nfrom inc import inc1, inc2\n\n# This test will work\ndef test_inc1():\n    assert inc1(3) == 4\n\n# This test will fail\ndef test_inc2():\n    assert inc2(-1) == 4\nTo run these tests:\n$ pytest test_inc.py\n\n\n\n\n\n\n### Exercise:\n\n\n1. Correct the test_inc2 test. 2. Determine the syntax to run any test in a directory. 3. Determine the syntax to run only the test called test_inc1.\n\n\n\n\n\n\n\npytest comes with some useful plugins. In particular, we will use the coverage report plugin.\nA test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs. A program with high test coverage, measured as a percentage, has had more of its source code executed during testing: this suggests it has a lower chance of containing undetected software bugs compared to a program with low test coverage.\nTo install the coverage plugin simply run\n$ pip install pytest-cov\nAssuming the inc_cov.py contains:\ndef inc(x):\n    if x < 0:\n        return 0\n    return x + 1\n\ndef dec(x):\n     return x - 1\nand a single test is performed through the file test_inc_cov.py\nfrom inc_cov import inc\n\ndef test_inc():\n     assert inc(3) == 4\nthen\npytest test_inc_cov.py --cov\n============================= test session starts ==============================\nplatform linux -- Python 3.8.5, pytest-6.2.5, py-1.9.0, pluggy-0.13.1\nrootdir: /home/jsalmon/Documents/Mes_cours/Montpellier/HAX712X/Courses/Test\nplugins: tornasync-0.6.0.post2, cov-3.0.0, jupyter-server-1.0.9\ncollected 0 items\n\n\n----------- coverage: platform linux, python 3.8.5-final-0 -----------\nName         Stmts   Miss  Cover\n--------------------------------\ninc_cov.py       6      4    33%\n--------------------------------\nTOTAL            6      4    33%\n\n============================ no tests ran in 0.02s =============================\nTwo lines in inc_cov module were not used. See\npytest --cov --cov-report=html test_inc_cov.py\n\n============================= test session starts ==============================\nplatform linux -- Python 3.8.5, pytest-6.2.5, py-1.9.0, pluggy-0.13.1\nrootdir: /home/jsalmon/Documents/Mes_cours/Montpellier/HAX712X/Courses/Test\nplugins: tornasync-0.6.0.post2, cov-3.0.0, jupyter-server-1.0.9\ncollected 0 items                                                              \n\n\n----------- coverage: platform linux, python 3.8.5-final-0 -----------\nCoverage HTML written to dir htmlcov\n\nfor details.\nThe documentation can be found at https://pytest-cov.readthedocs.io/en/latest/. Source for this text: https://en.wikipedia.org/wiki/Code_coverage.\n\n\n\n\n\n\n### Exercise:\n\n\n\n\nbiketrauma/init.py 4 0 100% biketrauma/io/Load_db.py 9 0 100% biketrauma/io/init.py 3 0 100% biketrauma/preprocess/init.py 0 0 100% biketrauma/preprocess/get_accident.py 9 0 100% biketrauma/tests/test_biketrauma.py 21 0 100% biketrauma/vis/init.py 0 0 100% biketrauma/vis/plot_location.py 6 4 33%\n\n\n\nTOTAL 52 4 92% ``` —-"
  },
  {
    "objectID": "Courses/TimeMemory/tp.html",
    "href": "Courses/TimeMemory/tp.html",
    "title": "Time and memory efficiency",
    "section": "",
    "text": "import time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nOther magic are %timeit, %matplotlib, %autoreload: cf. https://ipython.org/ipython-doc/3/interactive/magics.html https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\nn = 1000\nval = 5.4\nprint('fill')\n%timeit a = np.empty(n); a.fill(val)\n# This is a `magic` command, works only in interactive case (Ipython, Notebook, Jupyter lab etc.)\n# Alternative: uncomment below\n# get_ipython().run_line_magic('timeit', 'a = np.empty(n); a.fill(val)')\nprint('empty')\n%timeit a = np.empty(n); a[:] = val\nprint('full')\n%timeit a = np.full((n,), val)\nprint('ones')\n%timeit a = np.ones(n) * val\nprint('repeat')\n%timeit a = np.repeat(val, n)\n\n\n\nUse the time module: import time\nstart = time.time()\na = np.ones(n) * val\nend = time.time()\nprint(\"Time to execute the command: {0:.5f} s.\".format(end - start))\n\n\n\nSparse matrices are useful to to handle potentially huge matrices, that have only few non-zero coefficients. http://scipy-lectures.org/advanced/scipy_sparse/introduction.html#why-sparse-matrices\nhttps://rushter.com/blog/scipy-sparse-matrices/ http://cmdlinetips.com/2018/03/sparse-matrices-in-python-with-scipy/ Examples: - natural language processing: we encode the presence of a word from a dictionary (let’s say the set of French words) and we put 0 / 1 in case of absence / presence of a word. - One-hot encoding, used to represent categorical data as sparse binary vectors. - the discretization of a physical system where very distant influences are set to zero (e.g. heat diffusion, fluid mechanics, electro/magnetism, etc.) - graphs: graphs are naturally represented by adjacency or incidence matrices (cf. below), and therefore beyond the graphs, maps!\n\n\n\nhttps://docs.scipy.org/doc/scipy/reference/sparse.html#module-scipy.sparse - coo_matrix(arg1[, shape, dtype, copy]): A sparse matrix in COOrdinate format. - csc_matrix(arg1[, shape, dtype, copy]): Compressed Sparse Column matrix - csr_matrix(arg1[, shape, dtype, copy]): Compressed Sparse Row matrix\nfrom scipy import sparse\nfrom scipy.sparse import isspmatrix\n\nId = sparse.eye(3)\nprint(Id.toarray())\nprint(f'Q: Is the matrix Id is sparse?\\nA: {isspmatrix(Id)}')\n\nn1 = 29\nn2 = 29\nmat_rnd = sparse.rand(n1, n2, density=0.25, format=\"csr\",\n                      random_state=42)\nprint(mat_rnd.toarray())\nprint(f'Q: Is the matrix mat_rnd is sparse?\\nA: {isspmatrix(mat_rnd)}')\n\n\n\n\nv = np.random.rand(n2)\nmat_rnd@v\n\n\nA classical framework for the application of sparse matrices is with graphs: although the number of nodes can be huge, each node of a graph is in general not connected to all nodes. If we represent a graph by its adjacency matrix:\n## Definition: adjacency matrix: Suppose that G=(V,E) is a graph, where \\left|V\\right|=n. Suppose that the vertices of G are arbitrarily numbered v_1,\\ldots,v_n. The adjacency matrix A of G is the matrix n \\times n of general term:\nA_{{i,j}}=\n\\left\\{\n    \\begin{array}{rl}\n        1, & \\text{if } (v_i,v_j) \\in E \\\\\n        0, & \\text{o.w.}\n     \\end{array}\n\\right.\n\n\n\nUsage depends on the nature and structure of the data: - csc_matrix is more efficient for slicing by column - csr_matrix is more efficient for the row case.\nimport networkx as nx\nnx.__version__\nG = nx.Graph()\nG.add_edge('A', 'B', weight=4)\nG.add_edge('A', 'C', weight=3)\nG.add_edge('B', 'D', weight=2)\nG.add_edge('C', 'D', weight=4)\nG.add_edge('D', 'A', weight=2)\n\nmy_seed = 44\nnx.draw_networkx(G, with_labels=True,\n                 pos=nx.spring_layout(G, seed=my_seed))\n\nlabels = nx.get_edge_attributes(G, \"weight\")\nnx.draw_networkx_edge_labels(G,\n                             pos=nx.spring_layout(G, seed=my_seed),\n                             edge_labels=labels)\nnx.draw_networkx_edges(G, pos=nx.spring_layout(G, seed=my_seed),\n                       width=list(labels.values()))\nplt.axis('off')\nplt.show()\n\n\n\n\nshow edges weights on the graph\nchange the edge width to be proportional to the edge weights\n\nA = nx.adjacency_matrix(G)\nprint(isspmatrix(A))\nprint(A.todense())\nnx.shortest_path(G, 'C', 'B', weight='weight')\n\n\n\n\nLet G = (V,E) be a (non-oriented) graph with n vertices, V = [1,\\dots,n], and p edges, E = [1,\\dots,p]. The graph can be represented by its vertex-edge incidence matrix D^\\top \\in \\mathbb{R}^{p \\times n} defined by\n(D^\\top)_{{e,v}} =\n\\left\\{\n     \\begin{array}{rl}\n    + 1, & \\text{if } v = \\min(i,j) \\\\\n    -1, & \\text{si } v = \\max(i,j) \\\\\n    0, & \\text{sinon}\n  \\end{array}\n  \\right.\nwhere e = (i,j).\n\n\n\nThe matrix L=D D^\\top is the so-called graph Laplacian of G\nD = nx.incidence_matrix(G, oriented=True).T\nprint(isspmatrix(D))\nprint(D.todense())\n\n\n\nimport matplotlib.pylab as plt\ng = nx.karate_club_graph()\nfig, ax = plt.subplots(1, 1, figsize=(8, 6));\nnx.draw_networkx(g, ax=ax)\nplt.axis('off')\nplt.show()\nfig, ax = plt.subplots()\n\nA = nx.adjacency_matrix(g).T\n\nprint(A.todense())\nax = plt.spy(A)\n\nprint((g.number_of_edges() / g.number_of_nodes()**2) * 100)\n\n\n\n# https://andrewmellor.co.uk/blog/articles/2014/12/14/d3-networks/\n# https://github.com/brandomr/ner2sna\n\n# %%\n\nfrom IPython.display import HTML\n\n\n# %%\n\nget_ipython().run_cell_magic('HTML', '', \"\\n<iframe height=400px width=100% src='force.html'></iframe>\")\n\n\n\n\nOpen Street Map interfaced with Networkx, using the package osmnx: !!! Known bug: https://stackoverflow.com/questions/59658167/cannot-import-name-crs-from-pyproj-for-using-the-osmnx-library and https://stackoverflow.com/questions/60312055/python-getting-typeerror-argument-of-type-crs-is-not-iterable-with-osmnx so pick version 0.14 at least conda install osmnx>=0.14 or pip install osmnx>=0.10 For Windows users there might be some trouble with installing the fiona package, see: https://stackoverflow.com/questions/54734667/error-installing-geopandas-a-gdal-api-version-must-be-specified-in-anaconda and https://jingwen-z.github.io/how-to-install-python-module-fiona-on-windows-os/\n\n\n\n\npip install osmnx\npip install Rtree\nconda install -c conda-forge libspatialindex=1.9.3\npip install osmnx\nInstall all packages required up to fiona.\nconda install -c conda-forge geopandas\nsay yes to everything\nOnce done, launch pip install osmnx==1.0.1 You will also need to install the package folium\n\nimport folium\nmap_osm = folium.Map(location=[43.610769, 3.876716])\nmap_osm.add_child(folium.RegularPolygonMarker(location=[43.610769, 3.876716],\n                  fill_color='#132b5e', radius=5))\nmap_osm\nimport osmnx as ox\nox.utils.config(use_cache=True)  # caching large download\nox.__version__\nG = ox.graph_from_place('Montpellier, France', network_type='bike')\nprint(f\"nb edges: {G.number_of_edges()}\")\nprint(f\"nb nodes: {G.number_of_nodes()}\")\nox.plot_graph(G)\n\n\n\nhttps://blog.ouseful.info/2018/06/29/working-with-openstreetmap-roads-data-using-osmnx/\norigin = ox.geocoder.geocode('Place Eugène Bataillon, Montpellier, France')\ndestination = ox.geocoder.geocode('Maison du Lez, Montpellier, France')\n\norigin_node = ox.get_nearest_node(G, origin)\ndestination_node = ox.get_nearest_node(G, destination)\n\nprint(origin)\nprint(destination)\nroute = nx.shortest_path(G, origin_node, destination_node)\n# XXX double check if weights are taken into account.\nox.plot_graph_route(G, route)\nox.plot_route_folium(G, route, weight=5, color='#AA1111', opacity=0.7)\n# Adapted from : https://blog.ouseful.info/2018/06/29/working-with-openstreetmap-roads-data-using-osmnx/\nG.is_multigraph()\nedges = ox.graph_to_gdfs(G, nodes=False, edges=True)\nnodes = ox.graph_to_gdfs(G, nodes=True, edges=False)\n# Check columns\nprint(edges.columns)\nprint(nodes.columns)\nD = nx.incidence_matrix(G, oriented=True).T\nelement = np.zeros(1, dtype=float)\nmem = np.prod(D.shape) * element.data.nbytes / (1024**2)\nprint('Size of full matrix with zeros: {0:3.2f}  MB'.format(mem))\n\nprint('Size of sparse matrix: {0:3.2f}  MB'.format(D.data.nbytes/(1024**2) ))\n\nprint('Ratio  of full matrix size / sparse: {0:3.2f}%'.format(100 * D.data.nbytes / (1024**2 * mem)))\nprint(isspmatrix(D))\n```python\n\n\n**Alternatively**: you can uncomment the following line,\nand check that the size of a similar matrix, with non-sparse\nformat would be.\n```>>> Size of full matrix with zeros: 4 gB```\n\nCreation a matrix of similar size. BEWARE CREATE HUGE MATRIX:\n\n```python\nM = np.random.randn(G.number_of_nodes(), G.number_of_nodes())\nprint('Size of full matrix with zeros: {0:3.2f}  MB'.format(M.nbytes/(1024**2)))\n\n\nprint(\" {0:.2} % of edges only are needed to represent the graph of Montpellier\".format(100 * G.number_of_edges() / G.number_of_nodes() ** 2))\nTo go further on the visualization of geographical graphs: 1. https://geoffboeing.com/2016/11/osmnx-python-street-networks/ 2. https://automating-gis-processes.github.io/2017/lessons/L7/network-analysis.html 3. https://automating-gis-processes.github.io/2018/\nimport geopandas\ndf = geopandas.read_file(geopandas.datasets.get_path('nybb'))\nax = df.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\nplt.show()\n\n\n\nIn Python snakeviz could help, see https://jiffyclub.github.io/snakeviz/ In R, profvis is equivalent, see https://rstudio.github.io/profvis/\n\n\n\ninspired by https://davidhamann.de/2017/04/22/debugging-jupyter-notebooks/ Let us use import pdb; pdb.set_trace() to enter a code and inspect it. Push the key c and then enter to go next. See also: https://www.codementor.io/stevek/advanced-python-debugging-with-pdb-g56gvmpfa.\nA first recommendation is to use the Python debugger in your IDE. For VScode: see for instance https://www.youtube.com/watch?v=w8QHoVam1-I\nA possibility for pure Python or IPython is to use the pdb package and the command pdb.set_trace(). A command prompt launches when an error is met, and you can check the current status of the environnement. Useful shortcuts are available (e.g., touche c, touche j etc.) here: https://docs.python.org/3/library/pdb.html)\ndef function_debile(x):\n    answer = 42\n    answer += x\n    return answer\nfunction_debile(12)\ndef illustrate_pdb(x):\n    answer = 42\n    import pdb; pdb.set_trace()\n    answer += x\n    return answer\nillustrate_pdb(12)\npdb. A terminal is laucnhed when a problem occurs, and one can then take over and see what’s going on.\nget_ipython().run_line_magic('pdb', '')\n```python def blobl_func(x): answer = 0 for i in range(x, -1, -1): print(i) answer += 1 / i\nreturn answer\nblobl_func(4) ```python"
  },
  {
    "objectID": "Courses/Venv/tp.html",
    "href": "Courses/Venv/tp.html",
    "title": "Virtual Python Environment",
    "section": "",
    "text": "A venv is an isolated standalone python distribution with specific version of modules. This is useful when one need to run different python versions in a single system. There are various commands that are able to create a venv: venv, virtualenv, conda… We are going to use Anaconda to set up various python virtual environments on our system.\nMore information are available at: https://virtualenv.pypa.io/en/latest/, https://docs.python.org/3/library/venv.html\n\n\n\nThe Python Package Index (PyPI) is a repository of software for the Python programming language. PyPI helps you find and install software developed and shared by the Python community.\nThe pip program allows you to install.\n$ pip --version\n$ pip install numpy\nPlease have a look to https://pypi.org/, https://packaging.python.org/tutorials/installing-packages/ to solve the following\n\n\n\n\n\n\n### Exercise:\n\n\n1. Determine which python version there is on your system using locate and which 2. Determine which version of python is used by the pip command 3. List all the python modules installed with the pip command\n\n\n\n\n\n\nAnaconda is a package manager, an environment manager coming with a Python/R data science distribution, and a large collection of open-source packages. It is cross-platform and is a very popular choice in the data scientist community. Nevertheless, it suffers from a main drawback: it is heavy. Moreover, it comes with its own package manager conda which allows you to install python module (like pip) and other programs.\nOn the Linux box provided by the FdS, there is a terminal with the $PATH environment variable already configured (/net/apps/bin/init_anaconda3). You may launch it via the Graphical User Interface.\nRemark : See also the mamba project https://github.com/mamba-org/mamba\n\n\n\n\n\n\n### Exercise:\n\n\n1. Display the $PATH variable in the Anaconda_init terminal 2. Type conda deactivate and (re)-display the $PATH variable\n\n\n\n\n\nUse the terminal or an Anaconda Prompt for the following steps:\n\nTo create an environment:\n$ conda create --name myenv\nReplace myenv with the environment name.\nWhen conda asks you to proceed, type y:\n  proceed ([y]/n)?\nBy default, environments are installed into the envs sub-directory in your conda directory. See conda create --help for information on specifying a different path. This creates the myenv environment in envs/. This environment uses the same version of Python that you are currently using because you did not specify a version.\nTo create an environment with a specific version of Python:\n$ conda create -n myenv python=3.6\nTo create an environment with a specific package:\n$ conda create -n myenv scipy\nor:\n$ conda create -n myenv python\n$ conda install -n myenv scipy\nTo create an environment with a specific version of a package:\n$ conda create -n myenv scipy=0.15.0\nor\n$ conda create -n myenv python\n$ conda install -n myenv scipy=0.15.0\nTo create an environment with a specific version of Python and multiple packages:\n$ conda create -n myenv python=3.6 scipy=0.15.0 astroid babel\n\nSee: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands\n\n\n\n\n\n\n### Exercise:\n\n\n1. Create a new environment called toto with python3.5 and pandas version 0.23 2. Create another environment called tata with python3.7 and pandas version 1.0\n\n\n\n\n\n\nTo switch to an environment, it must be “activated” (in git we would have said “to checkout”). Activation entails two primary functions: adding entries to PATH for the environment and running any activation scripts that the environment may contain. These activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. You can also use the config API to set environment variables. To activate an environment:\n$ conda activate myenv\nChange myenv with the name on your environment.\nSee: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment\n\n\n\n\n\n\n### Exercise:\n\n\n1. Activate the toto environment. Launch python and check the version of pandas 2. Activate the tata environment. Launch python and check the version of pandas 3. List all the available environment (look in the documentation by yourself) 4. Come back to the base environment\n\n\n\n\n\n\nHave a look at https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#building-identical-conda-environments and do the following\n\n\n\n\n\n\n### Exercise:\n\n\n1. Imagine that you are coding a python module and a user is not able to run your code due to some missing dependencies. How can you help him to set up his python venv?\n\n\n\n\n\n\nAnaconda is particularly greedy in term of disk usage. It can be a good practice to remove an unused environment\n$ conda env remove -n myenv\nTo remove all cache and package run\n$ conda clean --all\n\n\n\n\n\n\n### Exercise:\n\n\n1. Remove all the environments created during this session 2. Create an environment called hmma238_env with matplotlib (this venv will be used in the next courses) 3. Clean the conda caches to free disk space."
  }
]